{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amoritized Inference for source-level variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gen;\n",
    "using Statistics: mean, std;\n",
    "using LinearAlgebra: dot;\n",
    "using StatsFuns: logsumexp;\n",
    "using PyPlot\n",
    "include(\"./time_helpers.jl\")\n",
    "include(\"./extra_distributions.jl\")\n",
    "include(\"./gaussian_helpers.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_params, steps, gtg_params, obs_noise = include(\"./base_params.jl\")\n",
    "scene_duration = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function wait_model()\n",
    "    \n",
    "    ##Element spacing \n",
    "    tp_latents = Dict(:wait=>Dict()) #just do wait for now\n",
    "    tp_params = source_params[\"tp\"]\n",
    "    for tp_type in keys(tp_latents)\n",
    "        hyperpriors = tp_params[String(tp_type)]\n",
    "        for latent in keys(hyperpriors)\n",
    "            hyperprior = hyperpriors[latent]; syml = Symbol(latent)\n",
    "            tp_latents[tp_type][syml] = @trace(hyperprior[\"dist\"](hyperprior[\"args\"]...), tp_type => syml)\n",
    "        end\n",
    "        tp_latents[tp_type][:args] = (tp_latents[tp_type][:a], tp_latents[tp_type][:mu]/tp_latents[tp_type][:a]) #a, b for gamma\n",
    "    end\n",
    "    \n",
    "    ne_params = source_params[\"n_elements\"]\n",
    "    n_elements = ne_params[\"type\"] == \"max\" ? \n",
    "        @trace(uniform_discrete(1, ne_params[\"val\"]),:n_elements) : \n",
    "        @trace(geometric(ne_params[\"val\"]),:n_elements)    \n",
    "    \n",
    "    waits = []\n",
    "    for element_idx = 1:n_elements\n",
    "        wait = element_idx == 1 ? @trace(uniform(0, scene_duration), (:element,element_idx)=>:wait) : \n",
    "            @trace(gamma(tp_latents[:wait][:args]...), (:element,element_idx)=>:wait)\n",
    "        push!(waits, wait)\n",
    "    end\n",
    "    \n",
    "    return waits\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function data_generator()\n",
    "    \n",
    "    trace = simulate(wait_model, ())\n",
    "    waits = get_retval(trace)\n",
    "    \n",
    "    constraints = choicemap()\n",
    "    constraints[:wait => :mu] = trace[:wait => :mu]\n",
    "    constraints[:wait => :a] = trace[:wait => :a]\n",
    "    \n",
    "    return ((waits,), constraints)\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "@gen function custom_dest_proposal_trainable(waits)\n",
    "    \n",
    "    #B_Qparameter\n",
    "    @param B_mu_mu::Vector{Float64}\n",
    "    @param B_logsigma_mu::Vector{Float64}\n",
    "    @param B_mu_alpha::Vector{Float64}\n",
    "    @param B_logsigma_alpha::Vector{Float64}\n",
    "    @param log_std_replace::Float64\n",
    "\n",
    "    n_w = length(waits)\n",
    "    mu_w = mean(waits)\n",
    "    sigma_w = n_w > 1.0 ? std(waits) : exp(log_std_replace)\n",
    "    X = [mu_w, sigma_w, n_w, 1]\n",
    "    \n",
    "    mu_mu = dot(X, B_mu_mu)\n",
    "    logsigma_mu = dot(X, B_logsigma_mu)\n",
    "    mu_alpha = dot(X, B_mu_alpha)\n",
    "    logsigma_alpha = dot(X, B_logsigma_alpha)\n",
    "    \n",
    "    @trace(log_normal(mu_mu,exp(logsigma_mu)),:wait => :mu)\n",
    "    @trace(log_normal(mu_alpha,exp(logsigma_alpha)),:wait => :a)\n",
    "    \n",
    "    return nothing\n",
    "    \n",
    "end\n",
    "\n",
    "for p in [:B_mu_mu, :B_logsigma_mu, :B_mu_alpha, :B_logsigma_alpha]\n",
    "    Gen.init_param!(custom_dest_proposal_trainable, p, zeros(4))\n",
    "end\n",
    "Gen.init_param!(custom_dest_proposal_trainable, :log_std_replace,0.)\n",
    "#Gen.ADAM(0.001, 0.9, 0.999, 1e-08)\n",
    "update = Gen.ParamUpdate(Gen.FixedStepGradientDescent(0.00001), custom_dest_proposal_trainable);\n",
    "scores = Gen.train!(custom_dest_proposal_trainable, data_generator, update,\n",
    "    num_epoch=100, epoch_size=1000, num_minibatch=1, \n",
    "    minibatch_size=1000, evaluation_size=100, verbose=true);\n",
    "\n",
    "plot(scores)\n",
    "xlabel(\"Iterations of stochastic gradient descent\")\n",
    "ylabel(\"Estimate of expected conditional log probability density\");\n",
    "title(\"Wait\")\n",
    "\n",
    "println(\"Wait Q params:\")\n",
    "println(\"B_mu_mu: \", Gen.get_param(custom_dest_proposal_trainable, :B_mu_mu))\n",
    "println(\"B_logsigma_mu: \", Gen.get_param(custom_dest_proposal_trainable, :B_logsigma_mu))\n",
    "println(\"B_mu_alpha: \", Gen.get_param(custom_dest_proposal_trainable, :B_mu_alpha))\n",
    "println(\"B_logsigma_alpha: \", Gen.get_param(custom_dest_proposal_trainable, :B_logsigma_alpha))\n",
    "println(\"log_std_replace: \", Gen.get_param(custom_dest_proposal_trainable, :log_std_replace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function durminusmin_model()\n",
    "    \n",
    "    ##Element spacing \n",
    "    tp_latents = Dict(:dur_minus_min=>Dict()) #just do wait for now\n",
    "    tp_params = source_params[\"tp\"]\n",
    "    for tp_type in keys(tp_latents)\n",
    "        hyperpriors = tp_params[String(tp_type)]\n",
    "        for latent in keys(hyperpriors)\n",
    "            hyperprior = hyperpriors[latent]; syml = Symbol(latent)\n",
    "            tp_latents[tp_type][syml] = @trace(hyperprior[\"dist\"](hyperprior[\"args\"]...), tp_type => syml)\n",
    "        end\n",
    "        tp_latents[tp_type][:args] = (tp_latents[tp_type][:a], tp_latents[tp_type][:mu]/tp_latents[tp_type][:a]) #a, b for gamma\n",
    "    end\n",
    "    \n",
    "    ne_params = source_params[\"n_elements\"]\n",
    "    n_elements = ne_params[\"type\"] == \"max\" ? \n",
    "        @trace(uniform_discrete(1, ne_params[\"val\"]),:n_elements) : \n",
    "        @trace(geometric(ne_params[\"val\"]),:n_elements)    \n",
    "    \n",
    "    dur_minus_mins = []\n",
    "    for element_idx = 1:n_elements\n",
    "        dur_minus_min = @trace(truncated_gamma(tp_latents[:dur_minus_min][:args]..., source_params[\"duration_limit\"]), (:element,element_idx)=>:dur_minus_min); \n",
    "        push!(dur_minus_mins, dur_minus_min)\n",
    "    end\n",
    "    \n",
    "    return dur_minus_mins\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function data_generator()\n",
    "    \n",
    "    trace = simulate(durminusmin_model, ())\n",
    "    dur_minus_mins = get_retval(trace)\n",
    "    \n",
    "    constraints = choicemap()\n",
    "    constraints[:dur_minus_min => :mu] = trace[:dur_minus_min => :mu]\n",
    "    constraints[:dur_minus_min => :a] = trace[:dur_minus_min => :a]\n",
    "    \n",
    "    return ((dur_minus_mins,), constraints)\n",
    "    \n",
    "end\n",
    "\n",
    "@gen function trainable_durminusmin_proposal(dur_minus_mins)\n",
    "    \n",
    "    #B_Qparameter\n",
    "    @param B_mu_mu::Vector{Float64}\n",
    "    @param B_logsigma_mu::Vector{Float64}\n",
    "    @param B_mu_alpha::Vector{Float64}\n",
    "    @param B_logsigma_alpha::Vector{Float64}\n",
    "    @param log_std_replace::Float64\n",
    "\n",
    "    n_d = length(dur_minus_mins)\n",
    "    mu_d = mean(dur_minus_mins)\n",
    "    sigma_d = n_d > 1.0 ? std(dur_minus_mins) : exp(log_std_replace)\n",
    "    X = [mu_d, sigma_d, n_d, 1]\n",
    "    \n",
    "    mu_mu = dot(X, B_mu_mu)\n",
    "    logsigma_mu = dot(X, B_logsigma_mu)\n",
    "    mu_alpha = dot(X, B_mu_alpha)\n",
    "    logsigma_alpha = dot(X, B_logsigma_alpha)\n",
    "    \n",
    "    @trace(log_normal(mu_mu,exp(logsigma_mu)),:dur_minus_min => :mu)\n",
    "    @trace(log_normal(mu_alpha,exp(logsigma_alpha)),:dur_minus_min => :a)\n",
    "    \n",
    "    return nothing\n",
    "    \n",
    "end\n",
    "\n",
    "for p in [:B_mu_mu, :B_logsigma_mu, :B_mu_alpha, :B_logsigma_alpha]\n",
    "    Gen.init_param!(trainable_durminusmin_proposal, p, zeros(4))\n",
    "end\n",
    "Gen.init_param!(trainable_durminusmin_proposal, :log_std_replace,0.)\n",
    "#Gen.ADAM(0.001, 0.9, 0.999, 1e-08)\n",
    "update = Gen.ParamUpdate(Gen.FixedStepGradientDescent(0.00001), trainable_durminusmin_proposal);\n",
    "scores = Gen.train!(trainable_durminusmin_proposal, data_generator, update,\n",
    "    num_epoch=100, epoch_size=1000, num_minibatch=1, \n",
    "    minibatch_size=1000, evaluation_size=100, verbose=false);\n",
    "\n",
    "plot(scores)\n",
    "xlabel(\"Iterations of stochastic gradient descent\")\n",
    "ylabel(\"Estimate of expected conditional log probability density\");\n",
    "title(\"Dur minus min\")\n",
    "println(\"Dur_minus_min Q params:\")\n",
    "println(\"B_mu_mu: \", Gen.get_param(custom_dest_proposal_trainable, :B_mu_mu))\n",
    "println(\"B_logsigma_mu: \", Gen.get_param(custom_dest_proposal_trainable, :B_logsigma_mu))\n",
    "println(\"B_mu_alpha: \", Gen.get_param(custom_dest_proposal_trainable, :B_mu_alpha))\n",
    "println(\"B_logsigma_alpha: \", Gen.get_param(custom_dest_proposal_trainable, :B_logsigma_alpha))\n",
    "println(\"log_std_replace: \", Gen.get_param(custom_dest_proposal_trainable, :log_std_replace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function gp_model(source_type, gp_type)\n",
    "    \n",
    "    ##GPs \n",
    "    gp_params = source_params[\"gp\"]\n",
    "    gp_latents = Dict(gp_type => Dict())\n",
    "    for gp_type in keys(gp_latents)\n",
    "        hyperpriors = gp_type === :erb ? gp_params[\"erb\"] : \n",
    "            ((source_type == \"noise\" || source_type == \"harmonic\") ? gp_params[\"amp\"][\"2D\"] : gp_params[\"amp\"][\"1D\"] )\n",
    "        for latent in keys(hyperpriors)\n",
    "            hyperprior = hyperpriors[latent]; syml = Symbol(latent)\n",
    "            gp_latents[gp_type][syml] = @trace(hyperprior[\"dist\"](hyperprior[\"args\"]...), gp_type => syml)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    ##Element spacing \n",
    "    tp_latents = Dict(:wait=>Dict(),:dur_minus_min=>Dict()) \n",
    "    tp_params = source_params[\"tp\"]\n",
    "    for tp_type in keys(tp_latents)\n",
    "        hyperpriors = tp_params[String(tp_type)]\n",
    "        for latent in keys(hyperpriors)\n",
    "            hyperprior = hyperpriors[latent]; syml = Symbol(latent)\n",
    "            tp_latents[tp_type][syml] = @trace(hyperprior[\"dist\"](hyperprior[\"args\"]...), tp_type => syml)\n",
    "        end\n",
    "        tp_latents[tp_type][:args] = (tp_latents[tp_type][:a], tp_latents[tp_type][:mu]/tp_latents[tp_type][:a]) #a, b for gamma\n",
    "    end\n",
    "    \n",
    "    ne_params = source_params[\"n_elements\"]\n",
    "    n_elements = ne_params[\"type\"] == \"max\" ? \n",
    "        @trace(uniform_discrete(1, ne_params[\"val\"]),:n_elements) : \n",
    "        @trace(geometric(ne_params[\"val\"]),:n_elements)    \n",
    "    \n",
    "    waits = []\n",
    "    dur_minus_mins = []\n",
    "    prev_gps = Dict(:x => [], gp_type => [])\n",
    "    time_so_far = 0.0;\n",
    "    for element_idx = 1:n_elements\n",
    "        wait = element_idx == 1 ? @trace(uniform(0, scene_duration), (:element,element_idx)=>:wait) : \n",
    "            @trace(gamma(tp_latents[:wait][:args]...), (:element,element_idx)=>:wait)\n",
    "        dur_minus_min = @trace(truncated_gamma(tp_latents[:dur_minus_min][:args]..., source_params[\"duration_limit\"]), (:element,element_idx)=>:dur_minus_min); \n",
    "\n",
    "        push!(waits, wait)\n",
    "        push!(dur_minus_mins, dur_minus_min)\n",
    "        duration = dur_minus_min + steps[\"min\"]; onset = time_so_far + wait; \n",
    "        time_so_far = onset + duration; element_timing = [onset, time_so_far]\n",
    "\n",
    "        ## Define points at which the GPs should be sampled\n",
    "        if gp_type === :erb || (source_type == \"tone\" && gp_type === :amp)\n",
    "            x = get_element_gp_times(element_timing, steps[\"t\"])\n",
    "        elseif gp_type === :amp && (source_type == \"noise\"  || source_type == \"harmonic\")\n",
    "            x, ts, fs = get_gp_spectrotemporal(element_timing, steps, audio_sr)\n",
    "        end\n",
    "\n",
    "        mu, cov = element_idx == 1 ? get_mu_cov(x, gp_latents[gp_type]) : \n",
    "                get_cond_mu_cov(x, prev_gps[:x], prev_gps[gp_type], gp_latents[gp_type])\n",
    "        element_gp = @trace(mvnormal(mu, cov), (:element, element_idx) => gp_type)\n",
    "        \n",
    "        append!(prev_gps[:x],x) #could also be push\n",
    "        append!(prev_gps[gp_type],element_gp)\n",
    "                \n",
    "    end\n",
    "    \n",
    "    return waits, dur_minus_mins, prev_gps\n",
    "    \n",
    "end\n",
    "\n",
    "source_type=\"tone\";gp_type=:amp;\n",
    "function data_generator()\n",
    "    \n",
    "    trace = simulate(gp_model, (source_type, gp_type))\n",
    "    waits, dur_minus_mins, prev_gps = get_retval(trace)\n",
    "    \n",
    "    constraints = choicemap()\n",
    "    d = gp_type == :erb ? source_params[\"gp\"][\"erb\"] : (source_type == \"tone\" ? source_params[\"gp\"][\"amp\"][\"1D\"] : source_params[\"gp\"][\"amp\"][\"2D\"]) \n",
    "    for k in keys(d)\n",
    "        constraints[gp_type => Symbol(k)] = trace[gp_type => Symbol(k)]\n",
    "    end\n",
    "    \n",
    "    return ((prev_gps,), constraints)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function trainable_amp1D_proposal(gps)\n",
    "\n",
    "    #Mu and Sigma of GP\n",
    "    @param B_mu_mu::Vector{Float64}\n",
    "    @param B_logsigma_mu::Vector{Float64}\n",
    "    @param B_mu_sigma::Vector{Float64}\n",
    "    @param B_logsigma_sigma::Vector{Float64}\n",
    "    @param log_std::Float64\n",
    "    \n",
    "    mu_d = means(gps[:amp])\n",
    "    sigma_d = (length(gps[:amp]) || all(y->y==gps[:amp][1], gps[:amp])) > 1 ? std(gps[:amp]) : exp(log_std_replace)\n",
    "    X = [mu_d, sigma_d, 1]\n",
    "    \n",
    "    mu_mu =dot(X, B_mu_mu)\n",
    "    logsigma_mu = dot(X, B_logsigma_mu)\n",
    "    mu_sigma = dot(X, B_mu_sigma)\n",
    "    logsigma_sigma = dot(X, B_logsigma_sigma)\n",
    "    \n",
    "    gp_mu = @trace(normal(mu_mu,exp(logsigma_mu)),:amp => :mu)\n",
    "    gp_sigma = @trace(log_normal(mu_sigma, exp(logsigma_sigma), :amp => :sigma))\n",
    "    \n",
    "    #Temporal lengthscale of GP\n",
    "    @param W_mu_scale::Vector{Float64}\n",
    "    @param C_mu_scale::Float64\n",
    "    @param replace_mu_scale::Float64\n",
    "    \n",
    "    @param W_logsigma_scale::Vector{Float64}\n",
    "    @param W_logsigma_scale::Float64\n",
    "    @param replace_logsigma_scale::Float64\n",
    "    \n",
    "    @param mu_times::Vector{Float64}\n",
    "    @param logsigma_times::Vector{Float64}\n",
    "    \n",
    "    if length(gps[:x]) > 1\n",
    "        \n",
    "        n = sum(1:length(gps[:x])-1)\n",
    "        delta_ts = Vector{Float64}(nothing, n)\n",
    "        delta_ys = Vector{Float64}(nothing, n)\n",
    "        rs = Vector{Float64}(nothing, n)\n",
    "        ps = Array{Float64}(nothing, n, 3)\n",
    "        k = 1\n",
    "        for i = 1:length(gps[:x])\n",
    "            for j = (i+1):length(gps[:x])\n",
    "                delta_ts[k] = gps[:x][j] - gps[:x][i]\n",
    "                delta_ys[k] = abs(gps[:amp][j] - gps[:amp][i])\n",
    "                rs[k] = delta_ys[k]/(delta_ts[k]*gp_sigma)\n",
    "                ps[k,:] = logsumexp([logpdf(normal, delta_ts[k], mu_times[g],exp(logsigma_times[g])) for g = 1:3])\n",
    "                k += 1\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        mu_scale = rs*p*W_mu_scale + C_mu_scale #mu_scale(1)=rs(n,)*ps(n,3)*W(3,)\n",
    "        logsigma_scale = rs*p*w_logsigma_scale + C_logsigma_scale \n",
    "        \n",
    "    else\n",
    "        \n",
    "        mu_scale = replace_mu_scale\n",
    "        logsigma_scale = replace_logsigma_scale\n",
    "        \n",
    "    end\n",
    "\n",
    "    @trace(log_normal(mu_scale, exp(logsigma_scale)), :amp=>:scale)\n",
    "    \n",
    "    ## epislon, local noise parameter of GP \n",
    "    @param replace_mu_noise::Float64\n",
    "    @param replace_logsigma_noise::Float64\n",
    "    \n",
    "    @trace(log_normal(mu_noise, exp(logsigma_noise)), :amp=>:noise)\n",
    "    \n",
    "end\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
