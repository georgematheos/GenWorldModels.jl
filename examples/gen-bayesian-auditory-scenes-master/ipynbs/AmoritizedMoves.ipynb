{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amoritized Inference for source-level variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gen;\n",
    "using Random;\n",
    "using Statistics: mean, std;\n",
    "using LinearAlgebra: dot;\n",
    "using StatsFuns: logsumexp;\n",
    "using PyPlot\n",
    "include(\"./time_helpers.jl\")\n",
    "include(\"./extra_distributions.jl\")\n",
    "include(\"./gaussian_helpers.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_params, steps, gtg_params, obs_noise = include(\"./base_params.jl\")\n",
    "scene_duration = 2.0\n",
    "println(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function wait_model()\n",
    "    \n",
    "    ##Element spacing \n",
    "    tp_latents = Dict(:wait=>Dict()) #just do wait for now\n",
    "    tp_params = source_params[\"tp\"]\n",
    "    for tp_type in keys(tp_latents)\n",
    "        hyperpriors = tp_params[String(tp_type)]\n",
    "        for latent in keys(hyperpriors)\n",
    "            hyperprior = hyperpriors[latent]; syml = Symbol(latent)\n",
    "            tp_latents[tp_type][syml] = @trace(hyperprior[\"dist\"](hyperprior[\"args\"]...), tp_type => syml)\n",
    "        end\n",
    "        tp_latents[tp_type][:args] = (tp_latents[tp_type][:a], tp_latents[tp_type][:mu]/tp_latents[tp_type][:a]) #a, b for gamma\n",
    "    end\n",
    "    \n",
    "    ne_params = source_params[\"n_elements\"]\n",
    "    n_elements = ne_params[\"type\"] == \"max\" ? \n",
    "        @trace(uniform_discrete(1, ne_params[\"val\"]),:n_elements) : \n",
    "        @trace(geometric(ne_params[\"val\"]),:n_elements)    \n",
    "    \n",
    "    waits = []\n",
    "    for element_idx = 1:n_elements\n",
    "        wait = element_idx == 1 ? @trace(uniform(0, scene_duration), (:element,element_idx)=>:wait) : \n",
    "            @trace(gamma(tp_latents[:wait][:args]...), (:element,element_idx)=>:wait)\n",
    "        push!(waits, wait)\n",
    "    end\n",
    "    \n",
    "    return waits\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function data_generator()\n",
    "    \n",
    "    trace = simulate(wait_model, ())\n",
    "    waits = get_retval(trace)\n",
    "    \n",
    "    constraints = choicemap()\n",
    "    constraints[:wait => :mu] = trace[:wait => :mu]\n",
    "    constraints[:wait => :a] = trace[:wait => :a]\n",
    "    \n",
    "    return ((waits,), constraints)\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "@gen function custom_dest_proposal_trainable(waits)\n",
    "    \n",
    "    #B_Qparameter\n",
    "    @param B_mu_mu::Vector{Float64}\n",
    "    @param B_logsigma_mu::Vector{Float64}\n",
    "    @param B_mu_alpha::Vector{Float64}\n",
    "    @param B_logsigma_alpha::Vector{Float64}\n",
    "    @param log_std_replace::Float64\n",
    "\n",
    "    n_w = length(waits)\n",
    "    mu_w = mean(waits)\n",
    "    sigma_w = n_w > 1.0 ? std(waits) : exp(log_std_replace)\n",
    "    X = [mu_w, sigma_w, n_w, 1]\n",
    "    \n",
    "    mu_mu = dot(X, B_mu_mu)\n",
    "    logsigma_mu = dot(X, B_logsigma_mu)\n",
    "    mu_alpha = dot(X, B_mu_alpha)\n",
    "    logsigma_alpha = dot(X, B_logsigma_alpha)\n",
    "    \n",
    "    @trace(log_normal(mu_mu,exp(logsigma_mu)),:wait => :mu)\n",
    "    @trace(log_normal(mu_alpha,exp(logsigma_alpha)),:wait => :a)\n",
    "    \n",
    "    return nothing\n",
    "    \n",
    "end\n",
    "\n",
    "for p in [:B_mu_mu, :B_logsigma_mu, :B_mu_alpha, :B_logsigma_alpha]\n",
    "    Gen.init_param!(custom_dest_proposal_trainable, p, zeros(4))\n",
    "end\n",
    "Gen.init_param!(custom_dest_proposal_trainable, :log_std_replace,0.)\n",
    "#Gen.ADAM(0.001, 0.9, 0.999, 1e-08)\n",
    "update = Gen.ParamUpdate(Gen.FixedStepGradientDescent(0.00001), custom_dest_proposal_trainable);\n",
    "scores = Gen.train!(custom_dest_proposal_trainable, data_generator, update,\n",
    "    num_epoch=100, epoch_size=1000, num_minibatch=1, \n",
    "    minibatch_size=1000, evaluation_size=100, verbose=true);\n",
    "\n",
    "plot(scores)\n",
    "xlabel(\"Iterations of stochastic gradient descent\")\n",
    "ylabel(\"Estimate of expected conditional log probability density\");\n",
    "title(\"Wait\")\n",
    "\n",
    "println(\"Wait Q params:\")\n",
    "println(\"B_mu_mu: \", Gen.get_param(custom_dest_proposal_trainable, :B_mu_mu))\n",
    "println(\"B_logsigma_mu: \", Gen.get_param(custom_dest_proposal_trainable, :B_logsigma_mu))\n",
    "println(\"B_mu_alpha: \", Gen.get_param(custom_dest_proposal_trainable, :B_mu_alpha))\n",
    "println(\"B_logsigma_alpha: \", Gen.get_param(custom_dest_proposal_trainable, :B_logsigma_alpha))\n",
    "println(\"log_std_replace: \", Gen.get_param(custom_dest_proposal_trainable, :log_std_replace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function durminusmin_model()\n",
    "    \n",
    "    ##Element spacing \n",
    "    tp_latents = Dict(:dur_minus_min=>Dict()) #just do wait for now\n",
    "    tp_params = source_params[\"tp\"]\n",
    "    for tp_type in keys(tp_latents)\n",
    "        hyperpriors = tp_params[String(tp_type)]\n",
    "        for latent in keys(hyperpriors)\n",
    "            hyperprior = hyperpriors[latent]; syml = Symbol(latent)\n",
    "            tp_latents[tp_type][syml] = @trace(hyperprior[\"dist\"](hyperprior[\"args\"]...), tp_type => syml)\n",
    "        end\n",
    "        tp_latents[tp_type][:args] = (tp_latents[tp_type][:a], tp_latents[tp_type][:mu]/tp_latents[tp_type][:a]) #a, b for gamma\n",
    "    end\n",
    "    \n",
    "    ne_params = source_params[\"n_elements\"]\n",
    "    n_elements = ne_params[\"type\"] == \"max\" ? \n",
    "        @trace(uniform_discrete(1, ne_params[\"val\"]),:n_elements) : \n",
    "        @trace(geometric(ne_params[\"val\"]),:n_elements)    \n",
    "    \n",
    "    dur_minus_mins = []\n",
    "    for element_idx = 1:n_elements\n",
    "        dur_minus_min = @trace(truncated_gamma(tp_latents[:dur_minus_min][:args]..., source_params[\"duration_limit\"]), (:element,element_idx)=>:dur_minus_min); \n",
    "        push!(dur_minus_mins, dur_minus_min)\n",
    "    end\n",
    "    \n",
    "    return dur_minus_mins\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function data_generator()\n",
    "    \n",
    "    trace = simulate(durminusmin_model, ())\n",
    "    dur_minus_mins = get_retval(trace)\n",
    "    \n",
    "    constraints = choicemap()\n",
    "    constraints[:dur_minus_min => :mu] = trace[:dur_minus_min => :mu]\n",
    "    constraints[:dur_minus_min => :a] = trace[:dur_minus_min => :a]\n",
    "    \n",
    "    return ((dur_minus_mins,), constraints)\n",
    "    \n",
    "end\n",
    "\n",
    "@gen function trainable_durminusmin_proposal(dur_minus_mins)\n",
    "    \n",
    "    #B_Qparameter\n",
    "    @param B_mu_mu::Vector{Float64}\n",
    "    @param B_logsigma_mu::Vector{Float64}\n",
    "    @param B_mu_alpha::Vector{Float64}\n",
    "    @param B_logsigma_alpha::Vector{Float64}\n",
    "    @param log_std_replace::Float64\n",
    "\n",
    "    n_d = length(dur_minus_mins)\n",
    "    mu_d = mean(dur_minus_mins)\n",
    "    sigma_d = n_d > 1.0 ? std(dur_minus_mins) : exp(log_std_replace)\n",
    "    X = [mu_d, sigma_d, n_d, 1]\n",
    "    \n",
    "    mu_mu = dot(X, B_mu_mu)\n",
    "    logsigma_mu = dot(X, B_logsigma_mu)\n",
    "    mu_alpha = dot(X, B_mu_alpha)\n",
    "    logsigma_alpha = dot(X, B_logsigma_alpha)\n",
    "    \n",
    "    @trace(log_normal(mu_mu,exp(logsigma_mu)),:dur_minus_min => :mu)\n",
    "    @trace(log_normal(mu_alpha,exp(logsigma_alpha)),:dur_minus_min => :a)\n",
    "    \n",
    "    return nothing\n",
    "    \n",
    "end\n",
    "\n",
    "for p in [:B_mu_mu, :B_logsigma_mu, :B_mu_alpha, :B_logsigma_alpha]\n",
    "    Gen.init_param!(trainable_durminusmin_proposal, p, zeros(4))\n",
    "end\n",
    "Gen.init_param!(trainable_durminusmin_proposal, :log_std_replace,0.)\n",
    "#Gen.ADAM(0.001, 0.9, 0.999, 1e-08)\n",
    "update = Gen.ParamUpdate(Gen.FixedStepGradientDescent(0.00001), trainable_durminusmin_proposal);\n",
    "scores = Gen.train!(trainable_durminusmin_proposal, data_generator, update,\n",
    "    num_epoch=100, epoch_size=1000, num_minibatch=1, \n",
    "    minibatch_size=1000, evaluation_size=100, verbose=false);\n",
    "\n",
    "plot(scores)\n",
    "xlabel(\"Iterations of stochastic gradient descent\")\n",
    "ylabel(\"Estimate of expected conditional log probability density\");\n",
    "title(\"Dur minus min\")\n",
    "println(\"Dur_minus_min Q params:\")\n",
    "println(\"B_mu_mu: \", Gen.get_param(custom_dest_proposal_trainable, :B_mu_mu))\n",
    "println(\"B_logsigma_mu: \", Gen.get_param(custom_dest_proposal_trainable, :B_logsigma_mu))\n",
    "println(\"B_mu_alpha: \", Gen.get_param(custom_dest_proposal_trainable, :B_mu_alpha))\n",
    "println(\"B_logsigma_alpha: \", Gen.get_param(custom_dest_proposal_trainable, :B_logsigma_alpha))\n",
    "println(\"log_std_replace: \", Gen.get_param(custom_dest_proposal_trainable, :log_std_replace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function gp_model(source_type, gp_type)\n",
    "    \n",
    "    ##GPs \n",
    "    gp_params = source_params[\"gp\"]\n",
    "    gp_latents = Dict(gp_type => Dict())\n",
    "    for gp_type in keys(gp_latents)\n",
    "        hyperpriors = gp_type === :erb ? gp_params[\"erb\"] : \n",
    "            ((source_type == \"noise\" || source_type == \"harmonic\") ? gp_params[\"amp\"][\"2D\"] : gp_params[\"amp\"][\"1D\"] )\n",
    "        for latent in keys(hyperpriors)\n",
    "            hyperprior = hyperpriors[latent]; syml = Symbol(latent)\n",
    "            gp_latents[gp_type][syml] = @trace(hyperprior[\"dist\"](hyperprior[\"args\"]...), gp_type => syml)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    ##Element spacing \n",
    "    tp_latents = Dict(:wait=>Dict(),:dur_minus_min=>Dict()) \n",
    "    tp_params = source_params[\"tp\"]\n",
    "    for tp_type in keys(tp_latents)\n",
    "        hyperpriors = tp_params[String(tp_type)]\n",
    "        for latent in keys(hyperpriors)\n",
    "            hyperprior = hyperpriors[latent]; syml = Symbol(latent)\n",
    "            tp_latents[tp_type][syml] = @trace(hyperprior[\"dist\"](hyperprior[\"args\"]...), tp_type => syml)\n",
    "        end\n",
    "        tp_latents[tp_type][:args] = (tp_latents[tp_type][:a], tp_latents[tp_type][:mu]/tp_latents[tp_type][:a]) #a, b for gamma\n",
    "    end\n",
    "    \n",
    "    ne_params = source_params[\"n_elements\"]\n",
    "    n_elements = ne_params[\"type\"] == \"max\" ? \n",
    "        @trace(uniform_discrete(1, ne_params[\"val\"]),:n_elements) : \n",
    "        @trace(geometric(ne_params[\"val\"]),:n_elements)    \n",
    "    \n",
    "    waits = []\n",
    "    dur_minus_mins = []\n",
    "    prev_gps = Dict(:x => [], :t => [], gp_type => [], :reshaped=>[])\n",
    "    time_so_far = 0.0;\n",
    "    for element_idx = 1:n_elements\n",
    "        wait = element_idx == 1 ? @trace(uniform(0, scene_duration), (:element,element_idx)=>:wait) : \n",
    "            @trace(gamma(tp_latents[:wait][:args]...), (:element,element_idx)=>:wait)\n",
    "        dur_minus_min = @trace(truncated_gamma(tp_latents[:dur_minus_min][:args]..., source_params[\"duration_limit\"]), (:element,element_idx)=>:dur_minus_min); \n",
    "\n",
    "        push!(waits, wait)\n",
    "        push!(dur_minus_mins, dur_minus_min)\n",
    "        duration = dur_minus_min + steps[\"min\"]; onset = time_so_far + wait; \n",
    "        time_so_far = onset + duration; element_timing = [onset, time_so_far]\n",
    "\n",
    "        ## Define points at which the GPs should be sampled\n",
    "        if gp_type === :erb || (source_type == \"tone\" && gp_type === :amp)\n",
    "            x = get_element_gp_times(element_timing, steps[\"t\"])\n",
    "        elseif gp_type === :amp && (source_type == \"noise\"  || source_type == \"harmonic\")\n",
    "            x, ts, prev_gps[:f] = get_gp_spectrotemporal(element_timing, steps, audio_sr)\n",
    "            append!(prev_gps[:t],ts)\n",
    "        end\n",
    "\n",
    "        mu, cov = element_idx == 1 ? get_mu_cov(x, gp_latents[gp_type]) : \n",
    "                get_cond_mu_cov(x, prev_gps[:x], prev_gps[gp_type], gp_latents[gp_type])\n",
    "        element_gp = @trace(mvnormal(mu, cov), (:element, element_idx) => gp_type)\n",
    "        if (source_type == \"harmonic\" || source_type == \"noise\") && (gp_type == :amp)\n",
    "            reshaped_gp = transpose(reshape(element_gp, (length(prev_gps[:f]), length(ts))))\n",
    "            push!(prev_gps[:reshaped],reshaped_gp)\n",
    "        end\n",
    "        \n",
    "        append!(prev_gps[:x],x) #could also be push\n",
    "        append!(prev_gps[gp_type],element_gp)\n",
    "                \n",
    "    end\n",
    "    \n",
    "    return waits, dur_minus_mins, prev_gps\n",
    "    \n",
    "end\n",
    "\n",
    "source_type=\"tone\";gp_type=:amp;\n",
    "function data_generator()\n",
    "    \n",
    "    trace = simulate(gp_model, (\"tone\", :amp))\n",
    "    waits, dur_minus_mins, prev_gps = get_retval(trace)\n",
    "    \n",
    "    constraints = choicemap()\n",
    "    d = gp_type == :erb ? source_params[\"gp\"][\"erb\"] : (source_type == \"tone\" ? source_params[\"gp\"][\"amp\"][\"1D\"] : source_params[\"gp\"][\"amp\"][\"2D\"]) \n",
    "    for k in keys(d)\n",
    "        constraints[gp_type => Symbol(k)] = trace[gp_type => Symbol(k)]\n",
    "    end\n",
    "    \n",
    "    return ((prev_gps,), constraints)\n",
    "    \n",
    "end\n",
    "\n",
    "data_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function trainable_amp1D_proposal(gps)\n",
    "    \n",
    "    #Mu and Sigma of GP\n",
    "    @param B_mu_mu::Vector{Float64}\n",
    "    @param B_logsigma_mu::Vector{Float64}\n",
    "    @param B_mu_sigma::Vector{Float64}\n",
    "    @param B_logsigma_sigma::Vector{Float64}\n",
    "    @param log_std_replace::Float64\n",
    "    \n",
    "    mu_d = mean(gps[:amp])\n",
    "    sigma_d = length(gps[:amp]) > 1 ? std(gps[:amp]) : exp(log_std_replace)\n",
    "    X = [mu_d, sigma_d, 1]\n",
    "    \n",
    "    mu_mu = dot(X, B_mu_mu)\n",
    "    logsigma_mu = dot(X, B_logsigma_mu)\n",
    "    mu_sigma = dot(X, B_mu_sigma)\n",
    "    logsigma_sigma = dot(X, B_logsigma_sigma)\n",
    "    \n",
    "    gp_mu = @trace(normal(mu_mu,exp(logsigma_mu)),:amp => :mu)\n",
    "    gp_sigma = @trace(log_normal(mu_sigma, exp(logsigma_sigma)), :amp => :sigma)\n",
    "    \n",
    "    #Temporal lengthscale of GP\n",
    "    @param W_mu_scale::Vector{Float64}\n",
    "    @param C_mu_scale::Float64\n",
    "    @param replace_mu_scale::Float64\n",
    "    \n",
    "    @param W_logsigma_scale::Vector{Float64}\n",
    "    @param C_logsigma_scale::Float64\n",
    "    @param replace_logsigma_scale::Float64\n",
    "    \n",
    "    @param mu_times::Vector{Float64}\n",
    "    @param logsigma_times::Vector{Float64}\n",
    "    \n",
    "    mu_scale = 0.0; logsigma_scale = 0.0;\n",
    "    len_gp = length(gps[:x])\n",
    "    if len_gp > 1\n",
    "        \n",
    "        #Subsample if GP is too big \n",
    "        #sample pairs instead of sampling points and getting the pairs within that\n",
    "        idx = []; max_len = 50\n",
    "        if len_gp > max_len\n",
    "            idx = Random.randsubseq(1:len_gp, min((max_len*1.5 * 1.0/len_gp),1.0))\n",
    "            idx = idx[1:min(max_len, length(idx))]\n",
    "        else\n",
    "            idx = 1:len_gp\n",
    "        end\n",
    "        \n",
    "        n = sum(1:length(idx)-1)\n",
    "        delta_ts = Vector{Float64}(undef, n)\n",
    "        delta_ys = Vector{Float64}(undef, n)\n",
    "        rs = Array{Float64}(undef, 1, n)\n",
    "        ps = Array{Float64}(undef, n, 3) #maybe defining this first is breaking the gradients? logsumexp? \n",
    "            \n",
    "        k = 1\n",
    "        for i = 1:length(idx)\n",
    "            for j = (i+1):length(idx)\n",
    "                i_idx = idx[i]; j_idx = idx[j]\n",
    "                delta_ts[k] = gps[:x][j_idx] - gps[:x][i_idx]\n",
    "                delta_ys[k] = abs(gps[:amp][j_idx] - gps[:amp][i_idx])\n",
    "                rs[k] = delta_ys[k]/(delta_ts[k]*gp_sigma)\n",
    "                lp = [Gen.logpdf(normal, delta_ts[k], mu_times[g],exp(logsigma_times[g])) for g = 1:3]\n",
    "                p = exp.(lp .- logsumexp(lp))\n",
    "                ps[k,:] = p\n",
    "                k += 1\n",
    "            end\n",
    "        end\n",
    "\n",
    "        mu_scale = (rs*ps*reshape(W_mu_scale, 3, 1))[1] + C_mu_scale #mu_scale(1)=rs(n,)*ps(n,3)*W(3,)\n",
    "        logsigma_scale = (rs*ps*reshape(W_logsigma_scale, 3, 1))[1] + C_logsigma_scale \n",
    "        \n",
    "    else\n",
    "\n",
    "        mu_scale = replace_mu_scale\n",
    "        logsigma_scale = replace_logsigma_scale\n",
    "        \n",
    "    end\n",
    "    @trace(log_normal(mu_scale, exp(logsigma_scale)), :amp=>:scale)\n",
    "    \n",
    "    ## epislon, local noise parameter of GP\n",
    "    #Could also calculate the deviation of a middle-point\n",
    "    #from the straight line between its neighbours\n",
    "    @param B_mu_noise::Vector{Float64}\n",
    "    @param replace_mu_noise::Float64\n",
    "    @param B_logsigma_noise::Vector{Float64}\n",
    "    @param replace_logsigma_noise::Float64\n",
    "    \n",
    "    local_stds = []\n",
    "    for i = 1:3:length(gps[:x])\n",
    "        t = gps[:x][i]\n",
    "        if issubset([t-steps[\"t\"], t, t+steps[\"t\"]], gps[:x])\n",
    "            local_gp = gps[:amp][i-1:i+1] #they have to be continguous\n",
    "            push!(local_stds, std(local_gp))\n",
    "        end\n",
    "    end\n",
    "    mu_noise = 0.0; logsigma_noise = 0.0;\n",
    "    if length(local_stds) > 1\n",
    "        mu_epsilon = mean(local_stds)\n",
    "        std_epsilon = std(local_stds)\n",
    "        X = [mu_epsilon, std_epsilon, 1]\n",
    "\n",
    "        mu_noise = dot(X, B_mu_noise)\n",
    "        logsigma_noise = dot(X, B_logsigma_noise)\n",
    "    else\n",
    "        mu_noise = replace_mu_noise\n",
    "        logsigma_noise = replace_logsigma_noise\n",
    "    end\n",
    "    @trace(log_normal(mu_noise, exp(logsigma_noise)), :amp=>:noise)\n",
    "    \n",
    "    return nothing\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in [:B_mu_mu,:B_logsigma_mu,:B_mu_sigma,:B_logsigma_sigma,:W_mu_scale,:W_logsigma_scale,:B_mu_noise,:B_logsigma_noise]\n",
    "    Gen.init_param!(trainable_amp1D_proposal, p, zeros(3))\n",
    "end\n",
    "#Gaussians:\n",
    "Gen.init_param!(trainable_amp1D_proposal, :mu_times, [0.01,0.8,2.0])\n",
    "Gen.init_param!(trainable_amp1D_proposal, :logsigma_times, [-2.0,-2.0,-2.0])\n",
    "for p in [:log_std_replace,:C_mu_scale,:replace_mu_scale,:C_logsigma_scale,:replace_logsigma_scale,:replace_mu_noise,:replace_logsigma_noise]\n",
    "    Gen.init_param!(trainable_amp1D_proposal, p, 0.0)\n",
    "end\n",
    "update = Gen.ParamUpdate(Gen.FixedStepGradientDescent(1e-8), trainable_amp1D_proposal);\n",
    "scores = Gen.train!(trainable_amp1D_proposal, data_generator, update,\n",
    "    num_epoch=100, epoch_size=50, num_minibatch=1, \n",
    "    minibatch_size=50, evaluation_size=10, verbose=false);\n",
    "\n",
    "plot(scores)\n",
    "xlabel(\"Iterations of stochastic gradient descent\")\n",
    "ylabel(\"Estimate of expected conditional log probability density\");\n",
    "title(\"GP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in [:B_mu_mu,:B_logsigma_mu,:B_mu_sigma,:B_logsigma_sigma,:W_mu_scale,:W_logsigma_scale,:mu_times,:logsigma_times,:B_mu_noise,:B_logsigma_noise,:log_std_replace,:C_mu_scale,:replace_mu_scale,:C_logsigma_scale,:replace_logsigma_scale,:replace_mu_noise,:replace_logsigma_noise]\n",
    "    s = string(p)\n",
    "    println(\"$s: \", Gen.get_param(trainable_amp1D_proposal, p))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_type=\"tone\";gp_type=:erb;\n",
    "function data_generator()\n",
    "    \n",
    "    trace = simulate(gp_model, (\"tone\", :erb))\n",
    "    waits, dur_minus_mins, prev_gps = get_retval(trace)\n",
    "    \n",
    "    constraints = choicemap()\n",
    "    d = gp_type == :erb ? source_params[\"gp\"][\"erb\"] : (source_type == \"tone\" ? source_params[\"gp\"][\"amp\"][\"1D\"] : source_params[\"gp\"][\"amp\"][\"2D\"]) \n",
    "    for k in keys(d)\n",
    "        constraints[gp_type => Symbol(k)] = trace[gp_type => Symbol(k)]\n",
    "    end\n",
    "    \n",
    "    return ((prev_gps,), constraints)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function trainable_erb_proposal(gps)\n",
    "    \n",
    "    #Mu and Sigma of GP\n",
    "    @param B_mu_mu::Vector{Float64}\n",
    "    @param B_logsigma_mu::Vector{Float64}\n",
    "    @param B_mu_sigma::Vector{Float64}\n",
    "    @param B_logsigma_sigma::Vector{Float64}\n",
    "    @param log_std_replace::Float64\n",
    "    \n",
    "    mu_d = mean(gps[:erb])\n",
    "    sigma_d = length(gps[:erb]) > 1 ? std(gps[:erb]) : exp(log_std_replace)\n",
    "    X = [mu_d, sigma_d, 1]\n",
    "    \n",
    "    mu_mu = dot(X, B_mu_mu)\n",
    "    logsigma_mu = dot(X, B_logsigma_mu)\n",
    "    mu_sigma = dot(X, B_mu_sigma)\n",
    "    logsigma_sigma = dot(X, B_logsigma_sigma)\n",
    "    \n",
    "    gp_mu = @trace(normal(mu_mu,exp(logsigma_mu)),:erb => :mu)\n",
    "    gp_sigma = @trace(log_normal(mu_sigma, exp(logsigma_sigma)), :erb => :sigma)\n",
    "    \n",
    "    #Temporal lengthscale of GP\n",
    "    @param W_mu_scale::Vector{Float64}\n",
    "    @param C_mu_scale::Float64\n",
    "    @param replace_mu_scale::Float64\n",
    "    \n",
    "    @param W_logsigma_scale::Vector{Float64}\n",
    "    @param C_logsigma_scale::Float64\n",
    "    @param replace_logsigma_scale::Float64\n",
    "    \n",
    "    @param mu_times::Vector{Float64}\n",
    "    @param logsigma_times::Vector{Float64}\n",
    "    \n",
    "    mu_scale = 0.0; logsigma_scale = 0.0;\n",
    "    len_gp = length(gps[:x])\n",
    "    if len_gp > 1\n",
    "        \n",
    "        #Subsample if GP is too big \n",
    "        idx = []; max_len = 50\n",
    "        if len_gp > max_len\n",
    "            idx = Random.randsubseq(1:len_gp, min((max_len*1.2 * 1.0/len_gp),1.0))\n",
    "            idx = idx[1:min(max_len, length(idx))]\n",
    "        else\n",
    "            idx = 1:len_gp\n",
    "        end\n",
    "        \n",
    "        n = sum(1:length(idx)-1)\n",
    "        delta_ts = Vector{Float64}(undef, n)\n",
    "        delta_ys = Vector{Float64}(undef, n)\n",
    "        rs = Array{Float64}(undef, 1, n)\n",
    "        ps = Array{Float64}(undef, n, 3)\n",
    "            \n",
    "        k = 1\n",
    "        for i = 1:length(idx)\n",
    "            for j = (i+1):length(idx)\n",
    "                i_idx = idx[i]; j_idx = idx[j]\n",
    "                delta_ts[k] = gps[:x][j_idx] - gps[:x][i_idx]\n",
    "                delta_ys[k] = abs(gps[:erb][j_idx] - gps[:erb][i_idx])\n",
    "                rs[k] = delta_ys[k]/(delta_ts[k]*gp_sigma)\n",
    "                lp = [Gen.logpdf(normal, delta_ts[k], mu_times[g],exp(logsigma_times[g])) for g = 1:3]\n",
    "                p = exp.(lp .- logsumexp(lp))\n",
    "                ps[k,:] = p\n",
    "                k += 1\n",
    "            end\n",
    "        end\n",
    "\n",
    "        mu_scale = (rs*ps*reshape(W_mu_scale, 3, 1))[1] + C_mu_scale #mu_scale(1)=rs(n,)*ps(n,3)*W(3,)\n",
    "        logsigma_scale = (rs*ps*reshape(W_logsigma_scale, 3, 1))[1] + C_logsigma_scale \n",
    "        \n",
    "    else\n",
    "\n",
    "        mu_scale = replace_mu_scale\n",
    "        logsigma_scale = replace_logsigma_scale\n",
    "        \n",
    "    end\n",
    "    @trace(log_normal(mu_scale, exp(logsigma_scale)), :erb=>:scale)\n",
    "    \n",
    "    ## epislon, local noise parameter of GP\n",
    "    #Could also calculate the deviation of a middle-point\n",
    "    #from the straight line between its neighbours\n",
    "    @param B_mu_noise::Vector{Float64}\n",
    "    @param replace_mu_noise::Float64\n",
    "    @param B_logsigma_noise::Vector{Float64}\n",
    "    @param replace_logsigma_noise::Float64\n",
    "    \n",
    "    local_stds = []\n",
    "    for i = 1:3:length(gps[:x])\n",
    "        t = gps[:x][i]\n",
    "        if issubset([t-steps[\"t\"], t, t+steps[\"t\"]], gps[:x])\n",
    "            local_gp = gps[:erb][i-1:i+1] #they have to be continguous\n",
    "            push!(local_stds, std(local_gp))\n",
    "        end\n",
    "    end\n",
    "    mu_noise = 0.0; logsigma_noise = 0.0;\n",
    "    if length(local_stds) > 1\n",
    "        mu_epsilon = mean(local_stds)\n",
    "        std_epsilon = std(local_stds)\n",
    "        X = [mu_epsilon, std_epsilon, 1]\n",
    "\n",
    "        mu_noise = dot(X, B_mu_noise)\n",
    "        logsigma_noise = dot(X, B_logsigma_noise)\n",
    "    else\n",
    "        mu_noise = replace_mu_noise\n",
    "        logsigma_noise = replace_logsigma_noise\n",
    "    end\n",
    "    @trace(log_normal(mu_noise, exp(logsigma_noise)), :erb=>:noise)\n",
    "    \n",
    "    return nothing\n",
    "    \n",
    "end\n",
    "\n",
    "for p in [:B_mu_mu,:B_logsigma_mu,:B_mu_sigma,:B_logsigma_sigma,:W_mu_scale,:W_logsigma_scale,:B_mu_noise,:B_logsigma_noise]\n",
    "    Gen.init_param!(trainable_erb_proposal, p, zeros(3))\n",
    "end\n",
    "#Gaussians:\n",
    "Gen.init_param!(trainable_erb_proposal, :mu_times, [0.01,0.8,2.0])\n",
    "Gen.init_param!(trainable_erb_proposal, :logsigma_times, [-2.0,-2.0,-2.0])\n",
    "for p in [:log_std_replace,:C_mu_scale,:replace_mu_scale,:C_logsigma_scale,:replace_logsigma_scale,:replace_mu_noise,:replace_logsigma_noise]\n",
    "    Gen.init_param!(trainable_erb_proposal, p, 0.0)\n",
    "end\n",
    "update = Gen.ParamUpdate(Gen.FixedStepGradientDescent(1e-8), trainable_erb_proposal);\n",
    "scores = Gen.train!(trainable_erb_proposal, data_generator, update,\n",
    "    num_epoch=100, epoch_size=50, num_minibatch=1, \n",
    "    minibatch_size=50, evaluation_size=10, verbose=false);\n",
    "\n",
    "plot(scores)\n",
    "xlabel(\"Iterations of stochastic gradient descent\")\n",
    "ylabel(\"Estimate of expected conditional log probability density\");\n",
    "title(\"erb GP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function trainable_amp2D_proposal(gps)\n",
    "    \n",
    "    #Mu and Sigma of GP\n",
    "    @param B_mu_mu::Vector{Float64}\n",
    "    @param B_logsigma_mu::Vector{Float64}\n",
    "    @param B_mu_sigma::Vector{Float64}\n",
    "    @param B_logsigma_sigma::Vector{Float64}\n",
    "    @param log_std_replace::Float64\n",
    "    \n",
    "    mu_d = mean(gps[:amp])\n",
    "    sigma_d = length(gps[:amp]) > 1 ? std(gps[:amp]) : exp(log_std_replace)\n",
    "    X = [mu_d, sigma_d, 1]\n",
    "    \n",
    "    mu_mu = dot(X, B_mu_mu)\n",
    "    logsigma_mu = dot(X, B_logsigma_mu)\n",
    "    mu_sigma = dot(X, B_mu_sigma)\n",
    "    logsigma_sigma = dot(X, B_logsigma_sigma)\n",
    "    \n",
    "    gp_mu = @trace(normal(mu_mu,exp(logsigma_mu)),:amp => :mu)\n",
    "    gp_sigma = @trace(log_normal(mu_sigma, exp(logsigma_sigma)), :amp => :sigma)\n",
    "    \n",
    "    #Temporal lengthscale of GP\n",
    "    @param W_mu_scale_t::Vector{Float64}\n",
    "    @param C_mu_scale_t::Float64\n",
    "    @param replace_mu_scale_t::Float64\n",
    "    \n",
    "    @param W_logsigma_scale_t::Vector{Float64}\n",
    "    @param C_logsigma_scale_t::Float64\n",
    "    @param replace_logsigma_scale_t::Float64\n",
    "    \n",
    "    @param mu_times_t::Vector{Float64}\n",
    "    @param logsigma_times_t::Vector{Float64}\n",
    "    \n",
    "    mu_scale = 0.0; logsigma_scale = 0.0;\n",
    "    len_gp = length(gps[:t])\n",
    "    if len_gp > 1\n",
    "        \n",
    "        #Subsample if GP is too big \n",
    "        idx = []; max_len = 50\n",
    "        if len_gp > max_len\n",
    "            idx = Random.randsubseq(1:len_gp, min((max_len*1.5 * 1.0/len_gp),1.0))\n",
    "            idx = idx[1:min(max_len, length(idx))]\n",
    "        else\n",
    "            idx = 1:len_gp\n",
    "        end\n",
    "        \n",
    "        n = sum(1:length(idx)-1)\n",
    "        delta_ts = Vector{Float64}(undef, n)\n",
    "        delta_ys = Vector{Float64}(undef, n)\n",
    "        rs = Array{Float64}(undef, 1, n)\n",
    "        ps = Array{Float64}(undef, n, 3)\n",
    "            \n",
    "        k = 1\n",
    "        for i = 1:length(idx)\n",
    "            for j = (i+1):length(idx)\n",
    "                i_idx = idx[i]; j_idx = idx[j]\n",
    "                delta_ts[k] = gps[:x][j_idx] - gps[:x][i_idx]\n",
    "                delta_ys[k] = abs(gps[:amp][j_idx] - gps[:amp][i_idx]) #correlation the two spectra\n",
    "                rs[k] = delta_ys[k]/(delta_ts[k]*gp_sigma)\n",
    "                lp = [Gen.logpdf(normal, delta_ts[k], mu_times_t[g],exp(logsigma_times_t[g])) for g = 1:3]\n",
    "                p = exp.(lp .- logsumexp(lp))\n",
    "                ps[k,:] = p\n",
    "                k += 1\n",
    "            end\n",
    "        end\n",
    "\n",
    "        mu_scale = (rs*ps*reshape(W_mu_scale_t, 3, 1))[1] + C_mu_scale_t #mu_scale(1)=rs(n,)*ps(n,3)*W(3,)\n",
    "        logsigma_scale = (rs*ps*reshape(W_logsigma_scale_t, 3, 1))[1] + C_logsigma_scale_t \n",
    "        \n",
    "    else\n",
    "\n",
    "        mu_scale = replace_mu_scale\n",
    "        logsigma_scale = replace_logsigma_scale\n",
    "        \n",
    "    end\n",
    "    @trace(log_normal(mu_scale, exp(logsigma_scale)), :amp=>:scale_t)\n",
    "    \n",
    "    ## epislon, local noise parameter of GP\n",
    "    #Could also calculate the deviation of a middle-point\n",
    "    #from the straight line between its neighbours\n",
    "    @param B_mu_noise::Vector{Float64}\n",
    "    @param replace_mu_noise::Float64\n",
    "    @param B_logsigma_noise::Vector{Float64}\n",
    "    @param replace_logsigma_noise::Float64\n",
    "    \n",
    "    local_stds = []\n",
    "    for i = 1:3:length(gps[:x])\n",
    "        t = gps[:x][i]\n",
    "        if issubset([t-steps[\"t\"], t, t+steps[\"t\"]], gps[:x])\n",
    "            local_gp = gps[:amp][i-1:i+1] #they have to be continguous\n",
    "            push!(local_stds, std(local_gp))\n",
    "        end\n",
    "    end\n",
    "    mu_noise = 0.0; logsigma_noise = 0.0;\n",
    "    if length(local_stds) > 1\n",
    "        mu_epsilon = mean(local_stds)\n",
    "        std_epsilon = std(local_stds)\n",
    "        X = [mu_epsilon, std_epsilon, 1]\n",
    "\n",
    "        mu_noise = dot(X, B_mu_noise)\n",
    "        logsigma_noise = dot(X, B_logsigma_noise)\n",
    "    else\n",
    "        mu_noise = replace_mu_noise\n",
    "        logsigma_noise = replace_logsigma_noise\n",
    "    end\n",
    "    @trace(log_normal(mu_noise, exp(logsigma_noise)), :amp=>:noise)\n",
    "    \n",
    "    return nothing\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort([3,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dict() \n",
    "for i = 1:10\n",
    "    if i == 1\n",
    "        x[:rs] = zeros(3, 4)\n",
    "    else\n",
    "        x[:rs] = cat(x[:rs], zeros(3,4), dims=2)\n",
    "    end\n",
    "end\n",
    "println(size(x[:rs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand(3:10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics: mean, std, cov;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov(rand(4,5),dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length(rand(4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics: I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
