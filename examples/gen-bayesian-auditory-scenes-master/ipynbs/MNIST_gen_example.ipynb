{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Random\n",
    "import Pkg; Pkg.add(\"MLDatasets\")\n",
    "import MLDatasets\n",
    "train_x, train_y = MLDatasets.MNIST.traindata()\n",
    "\n",
    "mutable struct MNISTTrainDataLoader\n",
    "    cur_id::Int\n",
    "    order::Vector{Int}\n",
    "end\n",
    "\n",
    "MNISTTrainDataLoader() = MNISTTrainDataLoader(1, Random.shuffle(1:60000))\n",
    "\n",
    "function next_batch(loader::MNISTTrainDataLoader, batch_size)\n",
    "    x = zeros(Float64, batch_size, 784)\n",
    "    y = Vector{Int}(undef, batch_size)\n",
    "    for i=1:batch_size\n",
    "        x[i, :] = reshape(train_x[:,:,loader.cur_id], (28*28))\n",
    "        y[i] = train_y[loader.cur_id] + 1\n",
    "        loader.cur_id += 1\n",
    "        if loader.cur_id > 60000\n",
    "            loader.cur_id = 1\n",
    "        end\n",
    "    end\n",
    "    x, y\n",
    "end\n",
    "\n",
    "function load_mnist_test_set()\n",
    "    test_x, test_y = MLDatasets.MNIST.testdata()\n",
    "    N = length(test_y)\n",
    "    x = zeros(Float64, N, 784)\n",
    "    y = Vector{Int}(undef, N)\n",
    "    for i=1:N\n",
    "        x[i, :] = reshape(test_x[:,:,i], (28*28))\n",
    "        y[i] = test_y[i]+1\n",
    "    end\n",
    "    x, y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gen\n",
    "using PyPlot\n",
    "\n",
    "# First, we load the GenTF package and the PyCall package. The PyCall package is used because TensorFlow computation graphs are constructed using the TensorFlow Python API, and the PyCall package allows Python code to be run from Julia.\n",
    "using PyCall\n",
    "using GenTF\n",
    "# We text load the TensorFlow and TensorFlow.nn Python modules into our scope. The `@pyimport` macro is defined by PyCall.\n",
    "tf = pyimport(\"tensorflow\")\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "nn = tf.nn\n",
    "\n",
    "# Next, we define a TensorFlow computation graph. The graph will have placeholders for an N x 784 matrix of pixel values, where N is the number of images that will be processed in batch, and 784 is the number of pixels in an MNIST image (28x28). There are 10 possible digit classes. The `probs` Tensor is an N x 10 matrix, where each row of the matrix is the vector of normalized probabilities of each digit class for a single input image. Note that this code is largely identical to the corresponding Python code. We provide initial values for the weight and bias parameters that are computed in Julia (it is also possible to use TensorFlow initializers for this purpose).\n",
    "# input images, shape (N, 784)\n",
    "xs = tf.compat.v1.placeholder(tf.float64, shape=(nothing, 784))\n",
    "\n",
    "# weight matrix parameter for soft-max regression, shape (784, 10)\n",
    "# initialize to a zeros matrix generated by Julia.\n",
    "init_W = zeros(Float64, 784, 10)\n",
    "W = tf.compat.v1.Variable(init_W)\n",
    "\n",
    "# bias vector parameter for soft-max regression, shape (10,)\n",
    "# initialize to a zeros vector generated by Julia.\n",
    "init_b = zeros(Float64, 10)\n",
    "b = tf.compat.v1.Variable(init_b)\n",
    "\n",
    "# probabilities for each class, shape (N, 10)\n",
    "probs = nn.softmax(tf.add(tf.matmul(xs, W), b), axis=1);\n",
    "\n",
    "# Next, we construct the generative function from this graph. The GenTF package provides a `TFFunction` type that implements the generative function interface. The `TFFunction` constructor takes:\n",
    "# (i) A vector of Tensor objects that will be the trainable parameters of the generative function (`[W, b]`). These should be TensorFlow variables.\n",
    "# (ii) A vector of Tensor object that are the inputs to the generative function (`[xs]`). These should be TensorFlow placeholders.\n",
    "# (iii) The Tensor object that is the return value of the generative function (`probs`).\n",
    "tf_softmax_model = TFFunction([W, b], [xs], probs);\n",
    "\n",
    "# The `TFFunction` constructor creates a new TensorFlow session that will be used to execute all TensorFlow code for this generative function. It is also TensorFlow possible to supply a session explicitly to the constructor. See the [GenTF documentation](https://probcomp.github.io/GenTF/dev/) for more details.\n",
    "# We can run the resulting generative function on some fake input data. This causes the TensorFlow to execute code in the TensorFlow session associated with `tf_softmax_model`:\n",
    "fake_xs = rand(5, 784)\n",
    "probs = tf_softmax_model(fake_xs)\n",
    "println(\"Size probs: \", size(probs))\n",
    "\n",
    "# We can also use `Gen.initialize` to obtain a trace of this generative function.\n",
    "(trace, _) = Gen.generate(tf_softmax_model, (fake_xs,));\n",
    "\n",
    "#  Note that generative functions constructed using GenTF do not make random choices:\n",
    "println(\"Get choices: \", Gen.get_choices(trace))\n",
    "\n",
    "# The return value is the Julia value corresponding to the Tensor `y`:\n",
    "println(\"Get size retval: \", size(Gen.get_retval(trace)))\n",
    "\n",
    "# Finally, we write a generative function using the built-in modeling DSL that invokes the TFFunction generative function we just defined. Note that we wrap the call to `tf_softmax_model` in an `@addr` statement.\n",
    "@gen function digit_model(xs::Matrix{Float64})\n",
    "    \n",
    "    # there are N input images, each with D pixels\n",
    "    (N, D) = size(xs)\n",
    "    \n",
    "    # invoke the `net` generative function to compute the digit label probabilities for all input images\n",
    "    probs = @trace(tf_softmax_model(xs), :softmax)\n",
    "    @assert size(probs) == (N, 10)\n",
    "    \n",
    "    # sample a digit label for each of the N input images\n",
    "    for i=1:N\n",
    "        @trace(categorical(probs[i,:]), (:y, i)) \n",
    "    end\n",
    "end;\n",
    "\n",
    "# Let's obtain a trace of `digit_model` on the fake tiny input:\n",
    "(trace, _) = Gen.generate(digit_model, (fake_xs,));\n",
    "# We see that the `net` generative function does not make any random choices. The only random choices are the digit labels for each input input:\n",
    "println(\"Digit model choices: \", Gen.get_choices(trace))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_loader = MNISTTrainDataLoader();\n",
    "\n",
    "# Now, we train the trainable parameters of the `tf_softmax_model` generative function  (`W` and `b`) on the MNIST traing data. Note that these parameters are stored as the state of the TensorFlow variables. We will use the [`Gen.train!`](https://probcomp.github.io/Gen/dev/ref/inference/#Gen.train!) method, which supports supervised training of generative functions using stochastic gradient opimization methods. In particular, this method takes the generative function to be trained (`digit_model`), a Julia function of no arguments that generates a batch of training data, and the update to apply to the trainable parameters.\n",
    "# The `ParamUpdate` constructor takes the type of update to perform (in this case a gradient descent update with step size 0.00001), and a specification of which trainable parameters should be updated). Here, we request that the `W` and `b` trainable parameters of the `tf_softmax_model` generative function should be trained.\n",
    "update = Gen.ParamUpdate(Gen.FixedStepGradientDescent(0.00001), tf_softmax_model => [W, b]);\n",
    "\n",
    "# For the data generator, we obtain a batch of 100 MNIST training images. The data generator must return a tuple, where the first element is a set of arguments to the generative function being trained (`(xs,)`) and the second element contains the values of random choices. `train!` attempts to maximize the expected log probability of these random choices given their corresponding input values.\n",
    "function data_generator()\n",
    "    (xs, ys) = next_batch(training_data_loader, 100)\n",
    "\n",
    "    @assert size(xs) == (100, 784)\n",
    "    @assert size(ys) == (100,)\n",
    "    constraints = Gen.choicemap()\n",
    "    for (i, y) in enumerate(ys)\n",
    "        constraints[(:y, i)] = y\n",
    "    end\n",
    "    ((xs,), constraints)\n",
    "end;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We run 10000 iterations of stochastic gradient descent, where each iteration uses a batch of 100 images to get a noisy gradient estimate. This might take one or two minutes.\n",
    "\n",
    "@time scores = Gen.train!(digit_model, data_generator, update;\n",
    "    num_epoch=1000, epoch_size=1, num_minibatch=1, minibatch_size=1, verbose=false);\n",
    "\n",
    "# We plot an estimate of the objective function function over time:\n",
    "\n",
    "plot(scores)\n",
    "xlabel(\"iterations of stochastic gradient descent\")\n",
    "ylabel(\"Estimate of expected conditional log likelihood\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
