{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "    \n",
    "#scene_t = get_element_gp_times([0,scene_duration], steps[\"t\"])\n",
    "batch_size=1; in_channels=2; \n",
    "in_height=None;#none;#could use scene_duration as input to \"define neural net\" then this equals length(scene_t); \n",
    "in_width=1; #width=frequency \n",
    "\n",
    "gp = tf.compat.v1.placeholder(dtype=tf.float64, shape=(batch_size, in_height, in_width, in_channels)) #tf.placeholder\n",
    "mask = tf.compat.v1.placeholder(dtype=tf.float64, shape=(batch_size, in_height, 1, 1))\n",
    "\n",
    "#Defining network\n",
    "filter_height = 3; filter_width = 1; \n",
    "out_channels = 4; n_params = 8;\n",
    "filters = [1,2,4]\n",
    "weights = []; layer_inputs = [gp];\n",
    "for i in range(len(filters)):\n",
    "    W = tf.compat.v1.get_variable(\"W{}\".format(i), \n",
    "            [filter_height, filter_width, \n",
    "            in_channels if i == 0 else out_channels, \n",
    "            n_params if i == len(filters)-1 else out_channels], \n",
    "            dtype=tf.float64, \n",
    "            initializer=tf.compat.v1.initializers.glorot_normal())\n",
    "    b = tf.compat.v1.get_variable(\"b{}\".format(i), \n",
    "            [n_params if i == len(filters)-1 else out_channels], \n",
    "            dtype=tf.float64, \n",
    "            initializer=tf.constant_initializer(value=0.0))\n",
    "    conv_dilations = [filters[i], 1]\n",
    "    L1 = tf.compat.v1.nn.convolution(layer_inputs[i], W, padding=\"SAME\", \n",
    "         dilation_rate=conv_dilations, data_format=\"NHWC\")\n",
    "    #L1 = tf.nn.conv2d(gp, W, (1,), padding=\"SAME\", data_format=\"NHWC\")\n",
    "    L1 = tf.nn.bias_add(L1, b, data_format=\"NHWC\")\n",
    "    C = tf.nn.relu(L1)\n",
    "    layer_inputs.append(C)\n",
    "\n",
    "#Convert maske convolutions to parameter estimates\n",
    "tile_mask = tf.tile(mask, (1, 1, 1, n_params))\n",
    "masked_conv = tf.multiply(tile_mask, layer_inputs[-1])\n",
    "O = tf.reduce_mean(masked_conv,axis=[1,2]) #batch_size, n_params, 1\n",
    "O = tf.nn.relu(O)\n",
    "Wf = tf.compat.v1.get_variable(\"Wfinal\",[1, n_params, n_params],dtype=tf.float64,\n",
    "        initializer=tf.compat.v1.initializers.glorot_normal())\n",
    "b = tf.compat.v1.get_variable(\"bfinal\",[n_params],dtype=tf.float64, \n",
    "    initializer=tf.constant_initializer(value=0.0)) #constant_initializer used to have dtype\n",
    "Wf = tf.tile(Wf, [batch_size, 1, 1])\n",
    "M = tf.matmul(Wf, tf.expand_dims(O,2))\n",
    "dist_parameters = tf.nn.bias_add(tf.squeeze(M,2), b)\n",
    "batch_loss = tf.reduce_sum(tf.abs(dist_parameters),axis=1)\n",
    "loss = tf.reduce_mean(batch_loss, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init=tf.compat.v1.global_variables_initializer()\n",
    "sess = tf.compat.v1.Session()\n",
    "sess.run(init)\n",
    "result = sess.run(loss, feed_dict={gp:np.ones((1,6,1,2)), mask:np.random.randn(1,6,1,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Result: {}\".format(result))\n",
    "print(\"Shape of output: {}\".format(np.shape(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = tf.compat.v1.trainable_variables()\n",
    "gradients = tf.compat.v1.gradients(loss, variables)\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(0.0001,beta1=0.9, beta2=0.999,epsilon=1e-8)\n",
    "train_op = optimizer.apply_gradients(zip(gradients, variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "loss_result, _ = sess.run([loss, train_op], feed_dict={gp:np.ones((1,6,1,2)), mask:np.random.randn(1,6,1,1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [];\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "for j in range(10000):\n",
    "    loss_result, d, _ = sess.run([loss, dist_parameters, train_op], feed_dict={gp:np.ones((1,6,1,2)), mask:np.random.randn(1,6,1,1)})\n",
    "    x.append(loss_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
