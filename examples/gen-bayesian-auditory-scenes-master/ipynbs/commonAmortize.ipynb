{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyCall #Put this first! \n",
    "using GenTF\n",
    "tf = pyimport(\"tensorflow\")\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "# add https://github.com/mcusi/GenTF\n",
    "# using Pkg; ENV[\"PYTHON\"] = \"/Users/maddiecusimano/Documents/basa-gen/env/bin/python\"; Pkg.build(\"PyCall\")\n",
    "# julia> using PyCall; println(PyCall.python)\n",
    "# /Users/maddiecusimano/.julia/conda/3/bin/python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gen;\n",
    "using Random;\n",
    "using Statistics: mean, std, cor;\n",
    "using LinearAlgebra: dot;\n",
    "using StatsFuns: logsumexp, softplus;\n",
    "using PyPlot\n",
    "using SpecialFunctions: digamma,trigamma;\n",
    "include(\"./time_helpers.jl\")\n",
    "include(\"./extra_distributions.jl\")\n",
    "include(\"./gaussian_helpers.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_params, steps, gtg_params, obs_noise = include(\"./base_params.jl\")\n",
    "audio_sr = 20000;\n",
    "scene_duration = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function make_source_latent_model(source_params, audio_sr, steps, scene_duration)\n",
    "    \n",
    "    @gen function source_latent_model(latents)\n",
    "\n",
    "        ##Single function for generating data for amoritized inference to propose source-level latents\n",
    "        # Wait: can generate on its own\n",
    "        # Dur_minus_min: can generate on its own\n",
    "        # GPs:\n",
    "        # Can have a separate amoritized inference move for ERB, Amp-1D, Amp-2D \n",
    "        # Need to sample wait and dur_minus_min to define the time points for the GP \n",
    "        #\n",
    "        # Format of latents: \n",
    "        # latents = Dict(:gp => :amp OR :tp => :wait, :source_type => \"tone\")\n",
    "\n",
    "        ### SOURCE-LEVEL LATENTS \n",
    "        ## Sample GP source-level latents if needed \n",
    "        gp_latents = Dict()\n",
    "        if :gp in keys(latents)\n",
    "\n",
    "            gp_params = source_params[\"gp\"]\n",
    "            gp_type = latents[:gp]; gp_latents[gp_type] = Dict();\n",
    "            source_type = latents[:source_type]\n",
    "            \n",
    "            hyperpriors = gp_type == :erb ? gp_params[\"erb\"] : \n",
    "                ((source_type == \"noise\" || source_type == \"harmonic\") ? gp_params[\"amp\"][\"2D\"] : gp_params[\"amp\"][\"1D\"] )\n",
    "    \n",
    "            for latent in keys(hyperpriors)\n",
    "                hyperprior = hyperpriors[latent]; syml = Symbol(latent)\n",
    "                gp_latents[gp_type][syml] = @trace(hyperprior[\"dist\"](hyperprior[\"args\"]...), gp_type => syml)\n",
    "            end\n",
    "            \n",
    "        end\n",
    "\n",
    "        ## Sample temporal source-level latents \n",
    "        tp_latents = Dict()\n",
    "        if :tp in keys(latents)\n",
    "            tp_latents[latents[:tp]] = Dict()\n",
    "        elseif :gp in keys(latents)\n",
    "            tp_latents[:wait] = Dict()\n",
    "            tp_latents[:dur_minus_min] = Dict()\n",
    "        end            \n",
    "        tp_params = source_params[\"tp\"]\n",
    "        for tp_type in keys(tp_latents)\n",
    "            hyperpriors = tp_params[String(tp_type)]\n",
    "            for latent in keys(hyperpriors)\n",
    "                hyperprior = hyperpriors[latent]; syml = Symbol(latent)\n",
    "                tp_latents[tp_type][syml] = @trace(hyperprior[\"dist\"](hyperprior[\"args\"]...), tp_type => syml)\n",
    "            end\n",
    "            tp_latents[tp_type][:args] = (tp_latents[tp_type][:a], tp_latents[tp_type][:mu]/tp_latents[tp_type][:a]) #a, b for gamma\n",
    "        end\n",
    "\n",
    "        ## Sample a number of elements \n",
    "        ne_params = source_params[\"n_elements\"]\n",
    "        n_elements = ne_params[\"type\"] == \"max\" ? \n",
    "            @trace(uniform_discrete(1, ne_params[\"val\"]),:n_elements) : \n",
    "            @trace(geometric(ne_params[\"val\"]),:n_elements)    \n",
    "\n",
    "        ### ELEMENT-LEVEL LATENTS \n",
    "        #Storage for what inputs are needed\n",
    "        tp_elems = Dict( [ k => [] for k in keys(tp_latents)]... )\n",
    "        gp_elems = Dict(); x_elems = [];\n",
    "        if :gp in keys(latents)       \n",
    "            \n",
    "            gp_type = latents[:gp]; source_type = latents[:source_type]\n",
    "            \n",
    "            gp_elems[gp_type] = []\n",
    "            gp_type = latents[:gp]\n",
    "            gp_elems[:t] = []\n",
    "            if gp_type == :amp && (source_type == \"noise\" || source_type == \"harmonic\")\n",
    "                gp_elems[:reshaped] = []\n",
    "                gp_elems[:f] = []\n",
    "                gp_elems[:tf] = []\n",
    "            end \n",
    "            \n",
    "        end\n",
    "                \n",
    "        time_so_far = 0.0;\n",
    "        for element_idx = 1:n_elements\n",
    "            \n",
    "            if :wait in keys(tp_elems)\n",
    "                wait = element_idx == 1 ? @trace(uniform(0, scene_duration), (:element,element_idx)=>:wait) : \n",
    "                    @trace(gamma(tp_latents[:wait][:args]...), (:element,element_idx)=>:wait)\n",
    "                push!(tp_elems[:wait], wait)\n",
    "            end\n",
    "            \n",
    "            if :dur_minus_min in keys(tp_elems)\n",
    "                dur_minus_min = @trace(truncated_gamma(tp_latents[:dur_minus_min][:args]..., source_params[\"duration_limit\"]), (:element,element_idx)=>:dur_minus_min); \n",
    "                push!(tp_elems[:dur_minus_min], dur_minus_min)\n",
    "            end\n",
    "            \n",
    "            if :gp in keys(latents)\n",
    "                \n",
    "                gp_type = latents[:gp]; source_type = latents[:source_type]\n",
    "                duration = dur_minus_min + steps[\"min\"]; onset = time_so_far + wait; \n",
    "                time_so_far = onset + duration; element_timing = [onset, time_so_far]\n",
    "                \n",
    "                if onset > scene_duration\n",
    "                    break\n",
    "                end\n",
    "                \n",
    "                ## Define points at which the GPs should be sampled\n",
    "                x = []; ts = [];\n",
    "                if gp_type === :erb || (source_type == \"tone\" && gp_type === :amp)\n",
    "                    x = get_element_gp_times(element_timing, steps[\"t\"])\n",
    "                elseif gp_type === :amp && (source_type == \"noise\"  || source_type == \"harmonic\")\n",
    "                    x, ts, gp_elems[:f] = get_gp_spectrotemporal(element_timing, steps, audio_sr)\n",
    "                end\n",
    "\n",
    "                mu, cov = element_idx == 1 ? get_mu_cov(x, gp_latents[gp_type]) : \n",
    "                        get_cond_mu_cov(x, x_elems, gp_elems[gp_type], gp_latents[gp_type])\n",
    "                element_gp = @trace(mvnormal(mu, cov), (:element, element_idx) => gp_type)\n",
    "                \n",
    "                ## Save the element data \n",
    "                append!(x_elems, x)\n",
    "                if gp_type === :erb || (source_type == \"tone\" && gp_type === :amp)\n",
    "                    append!(gp_elems[:t], x)\n",
    "                    append!(gp_elems[gp_type], element_gp)\n",
    "                elseif gp_type === :amp && (source_type == \"noise\"  || source_type == \"harmonic\")\n",
    "                    append!(gp_elems[:t],ts)\n",
    "                    append!(gp_elems[:tf],x) \n",
    "                    append!(gp_elems[gp_type], element_gp)\n",
    "                    reshaped_elem = reshape(element_gp, (length(gp_elems[:f]), length(ts))) \n",
    "                    if element_idx == 1\n",
    "                        gp_elems[:reshaped] = reshaped_elem\n",
    "                    else\n",
    "                        gp_elems[:reshaped] = cat(gp_elems[:reshaped], reshaped_elem, dims=2)\n",
    "                    end\n",
    "                end\n",
    "                \n",
    "                if time_so_far > scene_duration\n",
    "                    break\n",
    "                end\n",
    "           \n",
    "            end\n",
    "\n",
    "        end\n",
    "\n",
    "        return tp_latents, gp_latents, tp_elems, gp_elems\n",
    "\n",
    "    end\n",
    " \n",
    "    return source_latent_model\n",
    "    \n",
    "end\n",
    "source_latent_model = make_source_latent_model(source_params, audio_sr, steps, scene_duration);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function make_data_generator(source_latent_model, latents)\n",
    "    \n",
    "    function data_generator()\n",
    "\n",
    "        trace = simulate(source_latent_model, (latents,))\n",
    "        tp_latents, gp_latents, tp_elems, gp_elems = get_retval(trace)\n",
    "\n",
    "        constraints = choicemap()\n",
    "        if :tp in keys(latents)\n",
    "            tp_latent = latents[:tp]\n",
    "            constraints[tp_latent => :mu] = trace[tp_latent => :mu]\n",
    "            constraints[tp_latent => :a] = trace[tp_latent => :a]\n",
    "            \n",
    "        elseif :gp in keys(latents)\n",
    "            \n",
    "            gp_type = latents[:gp]\n",
    "            source_type = latents[:source_type]\n",
    "            d = gp_type == :erb ? source_params[\"gp\"][\"erb\"] : (source_type == \"tone\" ? source_params[\"gp\"][\"amp\"][\"1D\"] : source_params[\"gp\"][\"amp\"][\"2D\"]) \n",
    "            for k in keys(d)\n",
    "                constraints[gp_type => Symbol(k)] = trace[gp_type => Symbol(k)]\n",
    "            end\n",
    "            \n",
    "        end\n",
    "        inputs = :tp in keys(latents) ? (tp_elems,) : (gp_elems,)\n",
    "        \n",
    "        return (inputs, constraints)\n",
    "\n",
    "    end\n",
    "    \n",
    "    return data_generator\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function gen_gamma_model()\n",
    "    @trace(gamma(3,1),:sigma)\n",
    "    return rand()\n",
    "end\n",
    "function gamma_generator()\n",
    "    trace = simulate(gen_gamma_model, ())\n",
    "    r = get_retval(trace)\n",
    "    constraints = choicemap()\n",
    "    constraints[:sigma] = trace[:sigma]\n",
    "    return ((r,), constraints)\n",
    "end\n",
    "nonlinearity=exp; \n",
    "@gen function gen_gamma_proposal(r)\n",
    "    @param alpha::Float64\n",
    "    @param beta::Float64\n",
    "    a = nonlinearity(alpha); b=nonlinearity(beta);\n",
    "    @trace(gamma(a,b),:sigma)\n",
    "end\n",
    "Gen.init_param!(gen_gamma_proposal, :alpha, 0.0)\n",
    "Gen.init_param!(gen_gamma_proposal, :beta, 0.0)\n",
    "#GradientDescent(1e-3,10)\n",
    "update = Gen.ParamUpdate(Gen.FixedStepGradientDescent(1e-4), gen_gamma_proposal);\n",
    "scores = pain!(gen_gamma_proposal, gamma_generator, update,\n",
    "    num_epoch=5000, epoch_size=20, num_minibatch=1, \n",
    "    minibatch_size=20, evaluation_size=100, verbose=false);\n",
    "plot(scores)\n",
    "println(\"~~~~~~~~~\")\n",
    "println(\"~~~~~~~~~\")\n",
    "println(\"~~~~~~~~~\")\n",
    "for p in [:alpha, :beta]\n",
    "    s = string(p)\n",
    "    println(\"$s: \", nonlinearity(Gen.get_param(gen_gamma_proposal, p)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = Dict()\n",
    "latents[:wait] = Dict(:tp => :wait)\n",
    "latents[:dur_minus_min] = Dict(:tp => :dur_minus_min)\n",
    "latents[:amp1D] = Dict(:gp => :amp, :source_type => \"tone\")\n",
    "latents[:amp2D] = Dict(:gp => :amp, :source_type => \"noise\")\n",
    "latents[:erb] = Dict(:gp => :erb, :source_type => \"tone\")\n",
    "\n",
    "data_generators = Dict()\n",
    "for l in [:wait, :dur_minus_min, :amp1D, :amp2D, :erb]\n",
    "    data_generators[l] = make_data_generator(source_latent_model, latents[l])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_t = get_element_gp_times([0,scene_duration], steps[\"t\"]) \n",
    "net_inputs = []; masks = []; ms = []; ss = [];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_elem=data_generators[:amp2D]()[1][1]\n",
    "mask = [t in gp_elem[:t] ? 1.0 : 0.0 for t in scene_t]\n",
    "embedded_gp = []; k = 1; \n",
    "m = mean(gp_elem[:reshaped], dims=2); \n",
    "s = std(gp_elem[:reshaped]) #What about STD only in time direction--> (nf,) vector\n",
    "\n",
    "nf = size(gp_elem[:reshaped])[1]\n",
    "for t_idx in 1:length(scene_t)\n",
    "    if mask[t_idx] == 1\n",
    "        push!(embedded_gp, gp_elem[:reshaped][:,k])\n",
    "        k += 1\n",
    "    else\n",
    "        push!(embedded_gp, m)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_input = cat(embedded_gp..., dims=2)\n",
    "net_input = (net_input .- repeat(m, 1, length(scene_t))) ./ s\n",
    "push!(net_inputs, reshape(net_input, 1, size(net_input)...))\n",
    "\n",
    "mask = cat(mask...,dims=1)\n",
    "push!(masks, reshape(mask, 1, size(mask)...))\n",
    "\n",
    "push!(ms, m); push!(ss, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_net_input = cat(net_inputs..., dims=1)\n",
    "szn = size(batch_net_input)\n",
    "batch_net_input = reshape(batch_net_input, szn..., 1)\n",
    "batch_mask_input = cat(masks..., dims=1)\n",
    "szm = size(batch_mask_input) \n",
    "batch_mask_input = repeat(reshape(batch_mask_input, szm[1], 1, szm[2], 1), 1, szn[2], 1, 1)\n",
    "batch_net_input = cat(batch_net_input, batch_mask_input, dims=4)\n",
    "batch_net_input = permutedims(batch_net_input, [1, 3, 2, 4])\n",
    "batch_mask_input = permutedims(batch_mask_input, [1, 3, 2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat(mean(cat(embedded_gp..., dims=2),dims=2),1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutedims(cat(ms..., dims=2),[2,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_trainable_tp_proposal(latent)\n",
    "    \n",
    "    @gen function trainable_tp_proposal(tp_elems)\n",
    "\n",
    "        @param B_mean_mu::Vector{Float64}\n",
    "        @param B_shape_mu::Float64\n",
    "        @param C_shape_mu::Float64\n",
    "        @param B_mean_alpha::Vector{Float64}\n",
    "        @param B_shape_a::Float64\n",
    "        @param C_shape_a::Float64\n",
    "        @param MLE_estimate_alpha_0::Float64\n",
    "        @param MOM_estimate_alpha_0::Float64\n",
    "        \n",
    "        n_elements = length(tp_elems[latent])\n",
    "        if ( latent == :wait && n_elements > 1 ) || ( latent == :dur_minus_min )\n",
    "            \n",
    "            #For parameter estimation in Gamma distributions\n",
    "            #MLE estimators             \n",
    "            W = latent == :wait ? tp_elems[latent][2:end] : tp_elems[latent]\n",
    "            MLE_estimate_mu = mean(W)\n",
    "            if length(W) > 2    \n",
    "                s = log(mean(W)) - mean(log.(W))\n",
    "                MLE_estimate_alpha = (3.0 - s + sqrt((s-3)^2 + 24*s))/(12.0 * s) # initial guess\n",
    "                for i = 1:5 #iterative updates based on Newton-Raphson method\n",
    "                    numerator = log(MLE_estimate_alpha) - digamma(MLE_estimate_alpha) - s\n",
    "                    denominator = (1.0/MLE_estimate_alpha) - trigamma(MLE_estimate_alpha)\n",
    "                    MLE_estimate_alpha = MLE_estimate_alpha - (numerator/denominator)  \n",
    "                end\n",
    "            else\n",
    "                MLE_estimate_alpha =  exp(MLE_estimate_alpha_0)\n",
    "            end\n",
    "            \n",
    "            #Method of moments estimators \n",
    "            # MOM_estimate_mu = mean(W)\n",
    "            # MOM_alpha is pretty close to MLE_Estimate_alpha\n",
    "            MOM_estimate_alpha = length(W) > 2 ? (mean(W)^2)/(std(W)^2) : exp(MOM_estimate_alpha_0)\n",
    "                        \n",
    "            #For parameter estimation in log-normal distributions\n",
    "            #MLE_estimate_mu = mean(log(W))\n",
    "            #MLE_estimate_sigma = std(log(W))\n",
    "            #MOM_estimate_mu = -0.5*log(sum(W.^2)) + 2*log(sum(W)) - 1.5*log(length(W))\n",
    "            #MOM_estimate_sigma = log(sum(W.^2)) - 2*log(sum(W)) + log(length(W))\n",
    "            #X = [log(mu), log(mu^2/sigma^2), log(n), 1.0] #First two are method of moments\n",
    "\n",
    "            X_mu = [log(MLE_estimate_mu), 1.0]\n",
    "            X_alpha = [log(MLE_estimate_alpha), \n",
    "                     log(MOM_estimate_alpha),  \n",
    "                     1.0]            \n",
    "#             X_shapes = [log(MLE_estimate_alpha),\n",
    "#                        log(MOM_estimate_alpha), \n",
    "#                        log(mean([MLE_estimate_alpha, MOM_estimate_alpha])),\n",
    "#                        log(n_elements),\n",
    "#                        1.0]\n",
    "                        \n",
    "            mean_mu_estimate = exp( dot(X_mu, B_mean_mu) )\n",
    "            shape_mu = exp( B_shape_mu*n_elements + C_shape_mu)\n",
    "            mean_alpha_estimate = exp( dot(X_alpha, B_mean_alpha) )\n",
    "            shape_a = exp( B_shape_a*n_elements + C_shape_a )\n",
    "\n",
    "            @trace(gamma(shape_mu, mean_mu_estimate/shape_mu), latent => :mu)\n",
    "            #@trace(log_normal(log(mean_alpha_estimate), shape_a), latent=> :a)\n",
    "            @trace(gamma(shape_a, mean_alpha_estimate/shape_a), latent => :a)\n",
    "            #@trace(log_normal(log(mu_mu),exp(logsigma_mu)), latent => :mu) #Mean of wait dist\n",
    "            \n",
    "        else\n",
    "            \n",
    "            @trace(log_normal(-2.0,1.5), latent => :mu)\n",
    "            @trace(gamma(20,1), latent => :a)\n",
    "            \n",
    "        end\n",
    "\n",
    "        return nothing\n",
    "\n",
    "    end\n",
    "    \n",
    "    return trainable_tp_proposal\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_proposals = Dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_proposals[:wait] = create_trainable_tp_proposal(:wait)\n",
    "trainable_proposals[:dur_minus_min] = create_trainable_tp_proposal(:dur_minus_min);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = :dur_minus_min \n",
    "Gen.init_param!(trainable_proposals[latent], :B_mean_mu, [1.0, 0.0])\n",
    "# Gen.init_param!(trainable_proposals[latent], :B_shape_mu, zeros(5))\n",
    "Gen.init_param!(trainable_proposals[latent], :B_mean_alpha, zeros(3))\n",
    "# Gen.init_param!(trainable_proposals[latent], :B_shape_a, zeros(5))\n",
    "for p in [:MLE_estimate_alpha_0, :MOM_estimate_alpha_0, \n",
    "        :B_shape_mu, :B_shape_a, :C_shape_mu, :C_shape_a]\n",
    "    Gen.init_param!(trainable_proposals[latent], p, 0.0)\n",
    "end\n",
    "\n",
    "#Gen.FixedStepGradientDescent(1e-5)\n",
    "update = Gen.ParamUpdate(Gen.GradientDescent(1e-4,10), trainable_proposals[latent]);\n",
    "scores = pain!(trainable_proposals[latent], data_generators[latent], update,\n",
    "    num_epoch=5000, epoch_size=5, num_minibatch=1, \n",
    "    minibatch_size=5, evaluation_size=100, verbose=false);\n",
    "\n",
    "plot(scores)\n",
    "xlabel(\"Iterations of stochastic gradient descent\")\n",
    "ylabel(\"Estimate of expected conditional log probability density\");\n",
    "title(string(\"GP: \", string(latent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in [:B_mu_mu,:B_logsigma_mu,:B_mu_alpha,:B_logsigma_alpha,\n",
    "#           :log_std_replace]\n",
    "\n",
    "for p in [:B_mean_mu,:B_shape_mu,:B_mean_alpha,:B_shape_a,\n",
    ":MLE_estimate_alpha_0,:MOM_estimate_alpha_0,\n",
    "    :C_shape_mu, :C_shape_a]\n",
    "    s = string(p)\n",
    "    println(\"$s: \", Gen.get_param(trainable_proposals[latent], p))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gt_2 = Dict(:a=>Dict(:c=>[], :t=>[]),\n",
    "                :mu=>Dict(:c=>[], :t=>[]))\n",
    "n_points = 1000\n",
    "for z = 1:n_points\n",
    "    i,c = data_generators[latent]()\n",
    "    trace_list = Dict(:a=>[],:mu=>[])\n",
    "    if length(i[1][:dur_minus_min]) >= 2\n",
    "        for y = 1:10\n",
    "            trace, _, _ = propose(trainable_proposals[latent],(i[1],))\n",
    "            for a in keys(results_gt_2)\n",
    "                append!(trace_list[a],trace[latent=>a])\n",
    "            end\n",
    "        end\n",
    "        for a in keys(results_gt_2)\n",
    "            push!(results_gt_2[a][:c],c[latent=>a])\n",
    "            push!(results_gt_2[a][:t],mean(trace_list[a]))\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "results_lt_2 = Dict(:a=>Dict(:c=>[], :t=>[]),\n",
    "                :mu=>Dict(:c=>[], :t=>[]))\n",
    "n_points = 1000\n",
    "for z = 1:n_points\n",
    "    i,c = data_generators[latent]()\n",
    "    trace_list = Dict(:a=>[],:mu=>[])\n",
    "    if length(i[1][:dur_minus_min]) < 2\n",
    "        for y = 1:10\n",
    "            trace, _, _ = propose(trainable_proposals[latent],(i[1],))\n",
    "            for a in keys(results_lt_2)\n",
    "                append!(trace_list[a],trace[latent=>a])\n",
    "            end\n",
    "        end\n",
    "        for a in keys(results_lt_2)\n",
    "            push!(results_lt_2[a][:c],c[latent=>a])\n",
    "            push!(results_lt_2[a][:t],mean(trace_list[a]))\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(results_gt_2[:a][:c],results_gt_2[:a][:t])\n",
    "scatter(results_lt_2[:a][:c],results_lt_2[:a][:t])\n",
    "#ylim([-0.5,100])\n",
    "xlim([-0.5,50])\n",
    "xlabel(\"Actual\")\n",
    "ylabel(\"Predicted=mean(10 samples from proposal)\")\n",
    "title(\"$latent, alpha parameter, n=$n_points\")\n",
    "legend([\">=2elems\", \"<2elems\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(results_gt_2[:mu][:c],results_gt_2[:mu][:t])\n",
    "scatter(results_lt_2[:mu][:c],results_lt_2[:mu][:t])\n",
    "ylim([-0.5,7])\n",
    "xlim([-0.5,7])\n",
    "title(\"$latent, mu parameter, n=$n_points\")\n",
    "xlabel(\"Actual\")\n",
    "ylabel(\"Predicted=mean(10 samples from proposal)\")\n",
    "println(\"R for >=2elems: \", cor(results_gt_2[:mu][:c], results_gt_2[:mu][:t]))\n",
    "println(\"Max element duration (for truncated Gamma): \", source_params[\"duration_limit\"])\n",
    "legend([\">=2elems\", \"<2elems\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyCall #Put this first! \n",
    "using GenTF\n",
    "@pyimport tensorflow as tf\n",
    "\n",
    "# add https://github.com/mcusi/GenTF\n",
    "# using Pkg; ENV[\"PYTHON\"] = \"/Users/maddiecusimano/Documents/basa-gen/env/bin/python\"; Pkg.build(\"PyCall\")\n",
    "# julia> using PyCall; println(PyCall.python)\n",
    "# /Users/maddiecusimano/.julia/conda/3/bin/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_t = get_element_gp_times([0,3.5], steps[\"t\"])\n",
    "length(scene_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "        \n",
    "mask = tf.compat.v1.placeholder(dtype=tf.float64, shape=(1, 1, 6,))\n",
    "\n",
    "#Defining network\n",
    "Wy = tf.compat.v1.get_variable(\"Wy\",(1,1,6),initializer=tf.compat.v1.initializers.constant(0.0),dtype=tf.float64)\n",
    "# WL = tf.compat.v1.get_variable(\"WL\",[1,1,1],initializer=tf.compat.v1.initializers.glorot_normal(),dtype=tf.float64)\n",
    "# L1 = tf.compat.v1.nn.conv1d(mask, WL, (1,), padding=\"SAME\", data_format=\"NCW\")\n",
    "y = mask + Wy\n",
    "\n",
    "nn = TFFunction([Wy], [mask], y);\n",
    "\n",
    "x = [1.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n",
    "println(\"Input type: \", typeof(x))\n",
    "m = reshape(x, 1, 1, length(x))\n",
    "println(\"Size of m: \", size(m))\n",
    "y_out = nn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "        \n",
    "mask = tf.compat.v1.placeholder(dtype=tf.float64, shape=(1, 6, 1,))\n",
    "\n",
    "#Defining network\n",
    "WL = tf.compat.v1.get_variable(\"WL\",(1,1,6),initializer=tf.compat.v1.initializers.glorot_uniform(),dtype=tf.float64)\n",
    "L1 = tf.compat.v1.nn.conv1d(mask, WL, (1,), padding=\"SAME\", data_format=\"NWC\")\n",
    "\n",
    "nn = TFFunction([WL], [mask], L1);\n",
    "\n",
    "x = [1.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n",
    "println(\"Input type: \", typeof(x))\n",
    "m = reshape(x, 1, length(x), 1)\n",
    "println(\"Size of m: \", size(m))\n",
    "y_out = nn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n",
    "y = [2.0, 3.0, 4.0, 3.0, 3.0, 3.0]\n",
    "print(typeof(y))\n",
    "net_input = cat(x, y, dims=2)\n",
    "net_input = reshape(net_input, 1, length(x), 1, 2)\n",
    "println(size(net_input))\n",
    "net_mask = reshape(x, 1, length(x),1, 1)\n",
    "println(size(net_mask))\n",
    "tf.compat.v1.global_variables_initializer()\n",
    "params = nn(net_input, net_mask)\n",
    "println(params)\n",
    "(trace, _) = Gen.generate(nn, (net_input,net_mask,))\n",
    "get_choices(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "function define_neural_net(filter_height, filter_width, out_channels, n_params; dilations=[1,2,4])\n",
    "    \n",
    "    #scene_t = get_element_gp_times([0,scene_duration], steps[\"t\"])\n",
    "    batch_size=1; in_channels=2; \n",
    "    in_height=nothing;#none;#could use scene_duration as input to \"define neural net\" then this equals length(scene_t); \n",
    "    in_width=1; #width=frequency \n",
    "    \n",
    "    gp = tf.compat.v1.placeholder(dtype=tf.float64, \n",
    "        shape=(batch_size, in_height, in_width, in_channels)) #tf.placeholder\n",
    "    mask = tf.compat.v1.placeholder(dtype=tf.float64, shape=(batch_size, in_height, 1, 1))\n",
    "\n",
    "    #Defining network\n",
    "    #filter_height = 3; filter_width = 1; out_channels = 16; n_params = 8;\n",
    "    #dilations = [1,2,4]\n",
    "    weights = []; layer_inputs = [gp];\n",
    "    for i = 0:length(dilations)-1\n",
    "        ic = i == 0 ? in_channels : out_channels\n",
    "        oc = i == length(dilations) - 1 ? n_params : out_channels\n",
    "        W = tf.compat.v1.get_variable(\"W$i\", \n",
    "                (filter_height, filter_width, ic, oc), \n",
    "                dtype=tf.float64, \n",
    "                initializer=tf.compat.v1.initializers.glorot_normal())\n",
    "        b = tf.compat.v1.get_variable(\"b$i\", (oc), \n",
    "                dtype=tf.float64, \n",
    "                initializer=tf.constant_initializer(value=0.0))\n",
    "        append!(weights, [W, b])\n",
    "        L1 = tf.compat.v1.nn.convolution(layer_inputs[i+1], W, padding=\"SAME\", \n",
    "             dilation_rate=(dilations[i+1], 1), data_format=\"NHWC\")\n",
    "        #L1 = tf.nn.conv2d(gp, W, (1,), padding=\"SAME\", data_format=\"NHWC\")\n",
    "        L1 = tf.nn.bias_add(L1, b, data_format=\"NHWC\")\n",
    "        C = tf.nn.relu(L1)\n",
    "        push!(layer_inputs, C)\n",
    "    end\n",
    "    #Convert maske convolutions to parameter estimates\n",
    "    tile_mask = tf.tile(mask, (1, 1, 1, n_params))\n",
    "    masked_conv = tf.multiply(tile_mask, layer_inputs[end])\n",
    "    O = tf.reduce_sum(masked_conv,axis=(1,2)) #batch_size, n_params,\n",
    "    denom = tf.expand_dims(tf.reduce_sum(mask, axis=(1,2,3)),1)\n",
    "    O = tf.divide( O , denom ) \n",
    "    O = tf.nn.relu(O)\n",
    "#     Wf = tf.compat.v1.get_variable(\"Wfinal\",(1, n_params, n_params),dtype=tf.float64,\n",
    "#             initializer=tf.compat.v1.initializers.random_normal(stddev=1.0)) --> put this back and train longer?\n",
    "    Wf = tf.compat.v1.get_variable(\"Wfinal\",(1,n_params),dtype=tf.float64,\n",
    "              initializer=tf.compat.v1.initializers.random_normal(stddev=1.0))\n",
    "    b = tf.compat.v1.get_variable(\"bfinal\",(n_params),dtype=tf.float64, \n",
    "        initializer=tf.constant_initializer(value=0.0)) #put back to 1 and train longer?\n",
    "    append!(weights, [Wf, b])\n",
    "#     Wf = tf.tile(Wf, (batch_size, 1, 1))\n",
    "#     M = tf.matmul(Wf, tf.expand_dims(O,2))\n",
    "#     dist_parameters = tf.nn.bias_add(tf.squeeze(M,2), b)\n",
    "    dist_parameters = tf.nn.bias_add(tf.multiply(Wf,O),b)\n",
    "    \n",
    "    for w in weights\n",
    "        println(w)\n",
    "    end\n",
    "    \n",
    "    nn = GenTF.TFFunction(weights, [gp, mask], dist_parameters);\n",
    "    \n",
    "    return nn, weights\n",
    "    \n",
    "end\n",
    "nn,nn_weights = define_neural_net(5, 1, 16, 8, dilations=[1,2,4,8,16]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_trainable_gp1D_proposal(latent)\n",
    "\n",
    "    @gen function trainable_gp1D_proposal(gp_elems)\n",
    "\n",
    "        #format GP for input into neural network \n",
    "        scene_t = get_element_gp_times([0,scene_duration], steps[\"t\"]) #If training on variable length scenes, change this\n",
    "        mask = [t in gp_elems[:t] ? 1.0 : 0.0 for t in scene_t]\n",
    "        embedded_gp = []; k = 1; \n",
    "        m = mean(gp_elems[latent]); s = length(gp_elems[latent]) > 1 ? std(gp_elems[latent]) : 1.0\n",
    "        for t_idx in 1:length(scene_t)\n",
    "            if mask[t_idx] == 1\n",
    "                push!(embedded_gp, gp_elems[latent][k])\n",
    "                k += 1\n",
    "            else\n",
    "                push!(embedded_gp, m)\n",
    "            end\n",
    "        end\n",
    "        embedded_gp = embedded_gp .- m\n",
    "        net_input = cat(embedded_gp, mask, dims=2)\n",
    "        mask = cat(mask...,dims=1)\n",
    "        batch_size=1; in_channels=2; in_height=length(scene_t); in_width=1;\n",
    "        net_input = float.(reshape(net_input, batch_size, in_height, in_width, in_channels))\n",
    "        net_mask = float.(reshape(mask, batch_size, in_height, 1, 1))\n",
    "        dparams = @trace(nn(net_input,net_mask), :net_parameters)\n",
    "        \n",
    "        mean_mu_estimate = dparams[1]*m; shape_mu = softplus(dparams[2])\n",
    "        @trace(normal(mean_mu_estimate, shape_mu), latent => :mu) \n",
    "              \n",
    "        mean_sigma_estimate = softplus(dparams[3]) + length(gp_elems[latent]) > 1 ? s : 0.0; \n",
    "        shape_sigma = softplus(dparams[4])\n",
    "        @trace(gamma(shape_sigma, mean_sigma_estimate/shape_sigma), latent => :sigma)\n",
    "\n",
    "        mean_scale_estimate = softplus(dparams[5]); shape_scale = softplus(dparams[6]); \n",
    "        @trace(gamma(shape_scale, mean_scale_estimate/shape_scale), latent => :scale)        \n",
    "        \n",
    "        mean_epsilon_estimate = softplus(dparams[7]); shape_epsilon = softplus(dparams[8]); \n",
    "        @trace(gamma(shape_epsilon, mean_epsilon_estimate/shape_epsilon), latent => :epsilon)\n",
    "\n",
    "        return nothing\n",
    "\n",
    "    end\n",
    "\n",
    "    return trainable_gp1D_proposal\n",
    "    \n",
    "end\n",
    "trainable_proposals[:erb] = create_trainable_gp1D_proposal(:erb);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_size = 100\n",
    "evalsets=Dict()\n",
    "evalsets[:erb] = [data_generators[:erb]() for i = 1:evaluation_size];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = :erb \n",
    "\n",
    "#-3.6\n",
    "# update = Gen.ParamUpdate(Gen.FixedStepGradientDescent(1e-4),\n",
    "#      nn => collect(get_params(nn)));\n",
    "scores = pain!(trainable_proposals[latent], \n",
    "    data_generators[latent], \n",
    "    update,\n",
    "    num_epoch=10, epoch_size=2000, num_minibatch=500, \n",
    "    minibatch_size=10, evaluation_size=100, \n",
    "    eval_inputs_and_constraints=evalsets[latent], verbose=true);\n",
    "scatter(1:length(scores),scores,marker=\".\")\n",
    "xlabel(\"Iterations of stochastic gradient descent\")\n",
    "ylabel(\"Estimate of expected conditional log probability density\");\n",
    "title(string(\"GP net: \", string(latent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus(-0.000133382)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.compat.v1.train.Saver(Dict(string(var.name) => var for var in Gen.get_params(nn)))\n",
    "saver.save(GenTF.get_session(nn), \"./nn.ckpt\")\n",
    "saver.restore(GenTF.get_session(nn), \"./nn.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartoplot = :sigma\n",
    "n_points=1000\n",
    "actual=[]; empirical=[];single=[];\n",
    "for z = 1:n_points\n",
    "    i,c = data_generators[latent]()\n",
    "    gp = i[1][latent]\n",
    "    if length(gp) > 1\n",
    "        push!(empirical, std(gp))\n",
    "        push!(actual, c[latent=>vartoplot])\n",
    "    else\n",
    "        push!(single, c[latent=>vartoplot])\n",
    "    end\n",
    "end\n",
    "scatter(actual, empirical,marker=\".\")\n",
    "scatter(single,zeros(length(single)))\n",
    "legend([\">=2\",\"<2\"])\n",
    "xlabel(\"Sampled $vartoplot\")\n",
    "ylabel(\"STD calculated from GP\")\n",
    "title(\"$vartoplot compared to empirical std of gp\")\n",
    "maxy = maximum(empirical); miny=minimum(empirical);\n",
    "maxx = maximum(actual); minx =minimum(actual);\n",
    "xlim([min(miny,minx)-0.5, max(maxy,maxx)+0.5])\n",
    "ylim([min(miny,minx)-0.5, max(maxy,maxx)+0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 1000\n",
    "n_same = 1\n",
    "vartoplot = :sigma\n",
    "actual = []\n",
    "predicted = []\n",
    "for z = 1:n_points\n",
    "    i,c = data_generators[latent]()\n",
    "    push!(actual, c[latent=>vartoplot])\n",
    "    m = []\n",
    "    for j = 1:n_same\n",
    "        trace, _, _ = propose(trainable_proposals[latent],(i[1],))\n",
    "        push!(m, trace[latent=>vartoplot])\n",
    "    end\n",
    "    push!(predicted, mean(m))\n",
    "    #push!(predicted, std(i[1][:erb])) \n",
    "end\n",
    "scatter(actual,predicted,marker=\".\")\n",
    "xlabel(\"Actual\")\n",
    "ylabel(\"Mean of $n_same proposal samples\")\n",
    "r = round(cor(actual,predicted),digits = 4)\n",
    "title(\"$latent gp $vartoplot from neural network, r=$r, n=$n_points\")\n",
    "maxy = maximum(predicted); miny=minimum(predicted);\n",
    "maxx = maximum(actual); minx =minimum(actual);\n",
    "xlim([min(miny,minx)-0.5, max(maxy,maxx)+0.5])\n",
    "ylim([min(miny,minx)-0.5, max(maxy,maxx)+0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = get_session(nn)\n",
    "V = [var for var in tf.compat.v1.global_variables() if var.op.name==\"Wfinal\"]\n",
    "println(\"Wlinear: \",sess[:run](V[1]))\n",
    "v = [var for var in tf.compat.v1.global_variables() if var.op.name==\"bfinal\"]\n",
    "println(\"blinear: \", sess[:run](v[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = :scale\n",
    "h=[]\n",
    "gp_val = [20.0]; t_val=[0.22]\n",
    "n=length(gp_val)\n",
    "for i = 1:1000\n",
    "    choices,_,=propose(trainable_proposals[latent],(Dict(:erb=>gp_val,:t=>t_val),))\n",
    "    push!(h,choices[latent => a])\n",
    "end\n",
    "hist(h,bins=20,density=true)\n",
    "sp = source_params[\"gp\"][string(latent)][string(a)]\n",
    "xvals = a == :mu ? (0:50) : 0:0.05:6\n",
    "plot(xvals,exp.([Gen.logpdf(sp[\"dist\"],x,sp[\"args\"]...) for x in xvals]))\n",
    "legend([\"pdf\",\"samples\"])\n",
    "title(\"Hist $a with $n gp point vs. prior pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_trainable_gp1D_proposal(latent; max_t=50, check_n=2, max_locals=50)\n",
    "\n",
    "    @gen function trainable_gp1D_proposal(gp_elems)\n",
    "\n",
    "        #Mu and Sigma of GP\n",
    "        @param B_mu_mu::Vector{Float64}\n",
    "        @param B_logsigma_mu::Vector{Float64}\n",
    "        @param B_mu_sigma::Vector{Float64}\n",
    "        @param B_logsigma_sigma::Vector{Float64}\n",
    "        @param log_std_replace::Float64\n",
    "\n",
    "        mu = mean(gp_elems[latent])\n",
    "        sigma = length(gp_elems[latent]) > 1 ? std(gp_elems[latent]) : exp(log_std_replace)\n",
    "        X = [mu, sigma, length(gp_elems[latent]), 1]\n",
    "\n",
    "        mu_mu = dot(X, B_mu_mu)\n",
    "        logsigma_mu = dot(X, B_logsigma_mu)\n",
    "        mu_sigma = dot(X, B_mu_sigma)\n",
    "        logsigma_sigma = dot(X, B_logsigma_sigma)\n",
    "\n",
    "        gp_mu = @trace(normal(mu_mu,exp(logsigma_mu)), latent => :mu)\n",
    "        gp_sigma = @trace(log_normal(mu_sigma, exp(logsigma_sigma)), latent => :sigma)\n",
    "\n",
    "        #Temporal lengthscale of GP\n",
    "        @param W_mu_scale::Vector{Float64}\n",
    "        @param Bn_mu_scale::Float64\n",
    "        @param C_mu_scale::Float64\n",
    "        @param replace_mu_scale::Float64\n",
    "\n",
    "        @param W_logsigma_scale::Vector{Float64}\n",
    "        @param Bn_logsigma_scale::Float64\n",
    "        @param C_logsigma_scale::Float64\n",
    "        @param replace_logsigma_scale::Float64\n",
    "\n",
    "        @param mu_cats::Vector{Float64} #for categorizing time differences\n",
    "        @param logsigma_cats::Vector{Float64}\n",
    "\n",
    "        mu_scale = 0.0; logsigma_scale = 0.0; t = gp_elems[:t];\n",
    "        if length(t) > 1\n",
    "\n",
    "            #Subsample if GP is too big \n",
    "            idx = []; \n",
    "            if length(t) <= max_t\n",
    "                idx = 1:length(t) # 1 2 3 4\n",
    "            else\n",
    "                idx = Random.randperm(length(t)) # 4 7 2 1 8 5 3 6 10 9\n",
    "                idx = idx[1:max_t] # 4 7 2 1\n",
    "                idx = sort(idx) # 1 2 4 7 \n",
    "            end\n",
    "\n",
    "            n = sum(1:length(idx)-1) #how many pairs we will have \n",
    "            delta_ts = Vector{Float64}(undef, n)\n",
    "            delta_ys = Vector{Float64}(undef, n)\n",
    "            rs = Array{Float64}(undef, 1, n)\n",
    "            ps = [] #need to do it this way (rather than initialize an array) otherwise the gradients will break\n",
    "\n",
    "            k = 1\n",
    "            for i = 1:length(idx)\n",
    "                for j = (i+1):length(idx)\n",
    "                    i_idx = idx[i]; j_idx = idx[j]\n",
    "                    delta_ts[k] = t[j_idx] - t[i_idx]\n",
    "                    delta_ys[k] = abs(gp_elems[latent][j_idx] - gp_elems[latent][i_idx])\n",
    "                    rs[k] = delta_ys[k]/(delta_ts[k]*gp_sigma)\n",
    "                    lp = [Gen.logpdf(normal, delta_ts[k], mu_cats[g],exp(logsigma_cats[g])) for g = 1:3]\n",
    "                    p = exp.(lp .- logsumexp(lp))\n",
    "                    push!(ps, p)\n",
    "                    k += 1\n",
    "                end\n",
    "            end\n",
    "\n",
    "            ps = transpose(cat(ps...,dims=2))\n",
    "            mu_scale += (rs*ps*reshape(W_mu_scale, 3, 1))[1] + Bn_mu_scale*n + C_mu_scale #mu_scale(1)=rs(n,)*ps(n,3)*W(3,)\n",
    "            logsigma_scale += (rs*ps*reshape(W_logsigma_scale, 3, 1))[1] + Bn_logsigma_scale*n + C_logsigma_scale \n",
    "\n",
    "        else\n",
    "\n",
    "            mu_scale += replace_mu_scale\n",
    "            logsigma_scale += replace_logsigma_scale\n",
    "\n",
    "        end\n",
    "        @trace(log_normal(mu_scale, exp(logsigma_scale)), latent => :scale)\n",
    "\n",
    "        ## Epsilon, local noise parameter of GP\n",
    "        #Check stds between closeby points only to get estimate\n",
    "        #Could also calculate the deviation of a middle-point\n",
    "        #from the straight line between its neighbours\n",
    "        @param B_mu_epsilon::Vector{Float64}\n",
    "        @param replace_mu_epsilon::Float64\n",
    "        @param B_logsigma_epsilon::Vector{Float64}\n",
    "        @param replace_logsigma_epsilon::Float64\n",
    "\n",
    "        local_stds = []; segment_size = 2*check_n + 1\n",
    "        for i = 1:segment_size:length(t) #It won't go beyond bounds, even if length(t) < segment_size\n",
    "            if issubset( [ t[i]+k*steps[\"t\"] for k = -check_n:check_n ], t )\n",
    "                local_gp = gp_elems[latent][i-check_n:i+check_n] #contiguous GP points\n",
    "                push!(local_stds, std(local_gp))\n",
    "            end\n",
    "            if length(local_stds) == max_locals\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        mu_epsilon = 0.0; logsigma_epsilon = 0.0;\n",
    "        if length(local_stds) > 1\n",
    "            mu_locals = mean(local_stds)\n",
    "            std_locals = std(local_stds)\n",
    "            X = [mu_locals, std_locals, length(local_stds), 1]\n",
    "\n",
    "            mu_epsilon += dot(X, B_mu_epsilon)\n",
    "            logsigma_epsilon += dot(X, B_logsigma_epsilon)\n",
    "        else\n",
    "            mu_epsilon += replace_mu_epsilon\n",
    "            logsigma_epsilon += replace_logsigma_epsilon\n",
    "        end\n",
    "        \n",
    "        @trace(log_normal(mu_epsilon, exp(logsigma_epsilon)), latent => :epsilon)\n",
    "\n",
    "        return nothing\n",
    "\n",
    "    end\n",
    "\n",
    "    return trainable_gp1D_proposal\n",
    "    \n",
    "end\n",
    "trainable_proposals[:amp1D] = create_trainable_gp1D_proposal(:amp)\n",
    "trainable_proposals[:erb] = create_trainable_gp1D_proposal(:erb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = :erb \n",
    "for p in [:B_mu_mu,:B_logsigma_mu,:B_mu_sigma,:B_logsigma_sigma,\n",
    "          :B_mu_epsilon,:B_logsigma_epsilon]\n",
    "    Gen.init_param!(trainable_proposals[latent], p, zeros(4))\n",
    "end\n",
    "for p in [:W_mu_scale,:W_logsigma_scale]\n",
    "    Gen.init_param!(trainable_proposals[latent], p, zeros(3))\n",
    "end\n",
    "#Gaussians:\n",
    "Gen.init_param!(trainable_proposals[latent], :mu_cats, [0.01,0.8,2.0])\n",
    "Gen.init_param!(trainable_proposals[latent], :logsigma_cats, [-1.0,-1.0,-1.0])\n",
    "for p in [:log_std_replace,\n",
    "          :C_mu_scale,:replace_mu_scale,:C_logsigma_scale,:replace_logsigma_scale,\n",
    "          :Bn_mu_scale, :Bn_logsigma_scale,\n",
    "          :replace_mu_epsilon,:replace_logsigma_epsilon]\n",
    "    Gen.init_param!(trainable_proposals[latent], p, 0.0)\n",
    "end\n",
    "update = Gen.ParamUpdate(Gen.FixedStepGradientDescent(1e-8), trainable_proposals[latent]);\n",
    "scores = Gen.train!(trainable_proposals[latent], data_generators[latent], update,\n",
    "    num_epoch=50, epoch_size=50, num_minibatch=1, \n",
    "    minibatch_size=50, evaluation_size=10, verbose=false);\n",
    "\n",
    "plot(scores)\n",
    "xlabel(\"Iterations of stochastic gradient descent\")\n",
    "ylabel(\"Estimate of expected conditional log probability density\");\n",
    "title(string(\"GP: \", string(latent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in [:B_mu_mu,:B_logsigma_mu,:B_mu_sigma,:B_logsigma_sigma,\n",
    "          :B_mu_epsilon,:B_logsigma_epsilon,\n",
    "          :W_mu_scale,:W_logsigma_scale,\n",
    "          :log_std_replace,\n",
    "          :C_mu_scale,:replace_mu_scale,:C_logsigma_scale,:replace_logsigma_scale,\n",
    "          :Bn_mu_scale, :Bn_logsigma_scale,\n",
    "          :replace_mu_epsilon,:replace_logsigma_epsilon]\n",
    "    s = string(p)\n",
    "    println(\"$s: \", Gen.get_param(trainable_proposals[latent], p))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_trainable_gp2D_proposal(latent; max_t=5, check_nt=1, check_nf=2, max_locals=5)\n",
    "\n",
    "    @gen function trainable_gp2D_proposal(gp_elems)\n",
    "\n",
    "        #Mu and Sigma of GP\n",
    "        @param B_mu_mu::Vector{Float64}\n",
    "        @param B_logsigma_mu::Vector{Float64}\n",
    "        @param B_mu_sigma::Vector{Float64}\n",
    "        @param B_logsigma_sigma::Vector{Float64}\n",
    "\n",
    "        mu = mean(gp_elems[latent])\n",
    "        sigma = std(gp_elems[latent])\n",
    "        X = [mu, sigma, length(gp_elems[latent]), 1]\n",
    "\n",
    "        mu_mu = dot(X, B_mu_mu)\n",
    "        logsigma_mu = dot(X, B_logsigma_mu)\n",
    "        mu_sigma = dot(X, B_mu_sigma)\n",
    "        logsigma_sigma = dot(X, B_logsigma_sigma)\n",
    "\n",
    "        gp_mu = @trace(normal(mu_mu,exp(logsigma_mu)), latent => :mu)\n",
    "        gp_sigma = @trace(log_normal(mu_sigma, exp(logsigma_sigma)), latent => :sigma)\n",
    "        gp_elems[:reshaped] = gp_elems[:reshaped] .- gp_mu #de-mean\n",
    "\n",
    "        #Temporal lengthscale of GP\n",
    "        @param W_mu_scale_t::Vector{Float64}\n",
    "        @param Bn_mu_scale_t::Float64\n",
    "        @param C_mu_scale_t::Float64\n",
    "        @param replace_mu_scale_t::Float64\n",
    "\n",
    "        @param W_logsigma_scale_t::Vector{Float64}\n",
    "        @param Bn_logsigma_scale_t::Float64\n",
    "        @param C_logsigma_scale_t::Float64\n",
    "        @param replace_logsigma_scale_t::Float64\n",
    "\n",
    "        @param mu_cats::Vector{Float64} #for categorizing time differences\n",
    "        @param logsigma_cats::Vector{Float64}\n",
    "\n",
    "        mu_scale_t = 0.0; logsigma_scale_t = 0.0; t = gp_elems[:t];\n",
    "        if length(t) > 1\n",
    "\n",
    "            #Subsample if GP is too big \n",
    "            idx = []; \n",
    "            if length(t) <= max_t\n",
    "                idx = 1:length(t) # 1 2 3 4\n",
    "            else\n",
    "                idx = Random.randperm(length(t)) # 4 7 2 1 8 5 3 6 10 9\n",
    "                idx = idx[1:max_t] # 4 7 2 1\n",
    "                idx = sort(idx) # 1 2 4 7 \n",
    "            end\n",
    "\n",
    "            n = sum(1:length(idx)-1) #how many pairs we will have \n",
    "            delta_ts = Vector{Float64}(undef, n)\n",
    "            delta_ys = Vector{Float64}(undef, n)\n",
    "            rs = []#Array{Float64}(undef, 1, n)\n",
    "            ps = [] #need to do it this way (rather than initialize an array) otherwise the gradients will break\n",
    "\n",
    "            k = 1\n",
    "            for i = 1:length(idx)\n",
    "                for j = (i+1):length(idx)\n",
    "                    i_idx = idx[i]; j_idx = idx[j]\n",
    "                    delta_ts[k] = t[j_idx] - t[i_idx]\n",
    "                    #Differences between 2D and 1D case\n",
    "                    push!(rs, cor( gp_elems[:reshaped][:,j_idx] , gp_elems[:reshaped][:,i_idx] ))\n",
    "                    #delta_ys[k] = sqrt(sum((p1 .- p2).^2)) \n",
    "                    #rs[k] = delta_ys[k]/delta_ts[k]\n",
    "                    lp = [Gen.logpdf(normal, delta_ts[k], mu_cats[g],exp(logsigma_cats[g])) for g = 1:3]\n",
    "                    p = exp.(lp .- logsumexp(lp))\n",
    "                    push!(ps, p)\n",
    "                    k += 1\n",
    "                end\n",
    "            end\n",
    "            #cat(ps) is necessary to be able to compute the derivative given the list\n",
    "            #You need to use a list rather than editting a pre-made vector in place\n",
    "            ps = transpose(cat(ps...,dims=2))\n",
    "            rs = reshape(cat(rs...,dims=1), 1, n)\n",
    "            mu_scale_t += (rs*ps*reshape(W_mu_scale_t, 3, 1))[1] + Bn_mu_scale_t*n + C_mu_scale_t #mu_scale(1)=rs(n,)*ps(n,3)*W(3,)\n",
    "            logsigma_scale_t += (rs*ps*reshape(W_logsigma_scale_t, 3, 1))[1] + Bn_logsigma_scale_t*n + C_logsigma_scale_t \n",
    "\n",
    "        else\n",
    "\n",
    "            mu_scale_t += replace_mu_scale_t\n",
    "            logsigma_scale_t += replace_logsigma_scale_t\n",
    "\n",
    "        end\n",
    "        @trace(log_normal(mu_scale_t, exp(logsigma_scale_t)), latent => :scale_t)\n",
    "\n",
    "        #Frequency lengthscale of GP\n",
    "        @param B_mu_scale_f::Vector{Float64} #len = sum(1:length(f)-1)+2\n",
    "        @param B1_mu_scale_f::Vector{Float64}\n",
    "        @param B_logsigma_scale_f::Vector{Float64}\n",
    "        @param B1_logsigma_scale_f::Vector{Float64}\n",
    "        \n",
    "        f = gp_elems[:f]; \n",
    "        rs = [];\n",
    "        for i = 1:length(f)\n",
    "            for j = i+1:length(f)\n",
    "                if length(t) > 1\n",
    "                    r = cor( gp_elems[:reshaped][j,:], gp_elems[:reshaped][i,:] )\n",
    "                else\n",
    "                    dy = abs(gp_elems[:reshaped][j,1] - gp_elems[:reshaped][i,1])\n",
    "#                     df = f[j] - f[i]\n",
    "                    r = dy/gp_sigma\n",
    "                end\n",
    "                push!(rs, r)  \n",
    "            end\n",
    "        end\n",
    "        #rcat is necessary to be able to compute the derivative given the list\n",
    "        #You need to use a list rather than editting a pre-made vector in place\n",
    "        rcat = cat(rs..., length(t), 1.0, dims=1) \n",
    "        Bm = length(t) > 1 ? B_mu_scale_f : B1_mu_scale_f\n",
    "        Bs = length(t) > 1 ? B_logsigma_scale_f : B1_logsigma_scale_f\n",
    "        mu_scale_f = dot(Bm,rcat)\n",
    "        logsigma_scale_f = dot(Bs,rcat)\n",
    "        @trace(log_normal(mu_scale_f, exp(logsigma_scale_f)), latent => :scale_f)\n",
    "        \n",
    "        ## Epsilon, local noise parameter of GP\n",
    "        #Check stds between closeby points only to get estimate\n",
    "        #Could also calculate the deviation of a middle-point\n",
    "        #from the straight line between its neighbours\n",
    "        @param B_mu_epsilon::Vector{Float64}\n",
    "        @param replace_mu_epsilon::Float64\n",
    "        @param B_logsigma_epsilon::Vector{Float64}\n",
    "        @param replace_logsigma_epsilon::Float64\n",
    "\n",
    "        local_stds = []; segment_size_t = 2*check_nt + 1; \n",
    "        for i = 1:segment_size_t:length(t) #It won't go beyond bounds, even if length(t) < segment_size\n",
    "            if issubset( [ t[i]+k*steps[\"t\"] for k = -check_nt:check_nt ], t )\n",
    "                j = rand(1+check_nf:length(f)-check_nf)\n",
    "                local_gp = gp_elems[:reshaped][j-check_nf:j+check_nf, i-check_nt:i+check_nt][:] #contiguous GP points\n",
    "                push!(local_stds, std(local_gp))\n",
    "            end\n",
    "            if length(local_stds) == max_locals\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        mu_epsilon = 0.0; logsigma_epsilon = 0.0;\n",
    "        if length(local_stds) > 1\n",
    "            mu_locals = mean(local_stds)\n",
    "            std_locals = std(local_stds)\n",
    "            X = [mu_locals, std_locals, length(local_stds), 1.0]\n",
    "\n",
    "            mu_epsilon += dot(X, B_mu_epsilon)\n",
    "            logsigma_epsilon += dot(X, B_logsigma_epsilon)\n",
    "        else\n",
    "            mu_epsilon += replace_mu_epsilon\n",
    "            logsigma_epsilon += replace_logsigma_epsilon\n",
    "        end\n",
    "        @trace(log_normal(mu_epsilon, exp(logsigma_epsilon)), latent => :epsilon)\n",
    "\n",
    "        return nothing\n",
    "\n",
    "    end\n",
    "\n",
    "    return trainable_gp2D_proposal\n",
    "    \n",
    "end\n",
    "trainable_proposals[:amp2D] = create_trainable_gp2D_proposal(:amp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function pain!(gen_fn::GenerativeFunction, data_generator::Function,\n",
    "                update::ParamUpdate;\n",
    "                num_epoch=1, epoch_size=1, num_minibatch=1, minibatch_size=1,\n",
    "                evaluation_size=epoch_size, eval_inputs_and_constraints=[], verbose=false)\n",
    "\n",
    "    history = Vector{Float64}(undef, num_epoch)\n",
    "    \n",
    "    if length(eval_inputs_and_constraints) == 0 \n",
    "        println(\"Making evaluation dataset:\")\n",
    "        for i = 1:evaluation_size\n",
    "            print(\"$i \")\n",
    "            ic = data_generator()\n",
    "            push!(eval_inputs_and_constraints, ic)\n",
    "        end\n",
    "        println()\n",
    "    else\n",
    "        evaluation_size = length(eval_inputs_and_constraints)\n",
    "        println(\"Using eval set of size $evaluation_size\")\n",
    "    end\n",
    "    \n",
    "    for epoch=1:num_epoch\n",
    "\n",
    "        s1=time()\n",
    "        # generate data for epoch\n",
    "        if verbose\n",
    "            println(\"epoch $epoch: generating $epoch_size training examples...\")\n",
    "        end\n",
    "        epoch_inputs = Vector{Tuple}(undef, epoch_size)\n",
    "        epoch_choice_maps = Vector{ChoiceMap}(undef, epoch_size)\n",
    "        for i=1:epoch_size\n",
    "            (epoch_inputs[i], epoch_choice_maps[i]) = data_generator()\n",
    "        end\n",
    "        s2=time()-s1\n",
    "        println(\"Time to generate data: $s2\")\n",
    "\n",
    "        # train on epoch data\n",
    "        s1=time()\n",
    "        if verbose\n",
    "            println(\"epoch $epoch: training using $num_minibatch minibatches of size $minibatch_size...\")\n",
    "        end\n",
    "        for minibatch=1:num_minibatch\n",
    "            permuted = Random.randperm(epoch_size)\n",
    "            minibatch_idx = permuted[1:minibatch_size]\n",
    "            minibatch_inputs = epoch_inputs[minibatch_idx]\n",
    "            minibatch_choice_maps = epoch_choice_maps[minibatch_idx]\n",
    "            for (inputs, constraints) in zip(minibatch_inputs, minibatch_choice_maps)\n",
    "                (trace, _) = generate(gen_fn, inputs, constraints)\n",
    "                retval_grad = accepts_output_grad(gen_fn) ? zero(get_retval(trace)) : nothing\n",
    "                accumulate_param_gradients!(trace, retval_grad)\n",
    "            end\n",
    "            apply!(update)\n",
    "        end\n",
    "        s2=time()-s1\n",
    "        println(\"Time to train: $s2\")\n",
    "\n",
    "        # evaluate score on held out data\n",
    "        s1=time()\n",
    "        if verbose\n",
    "            println(\"epoch $epoch: evaluating on $evaluation_size examples...\")\n",
    "        end\n",
    "        avg_score = 0.\n",
    "        for i=1:evaluation_size\n",
    "            (_, weight) = generate(gen_fn, eval_inputs_and_constraints[i][1], eval_inputs_and_constraints[i][2])\n",
    "            avg_score += weight\n",
    "        end\n",
    "        avg_score /= evaluation_size\n",
    "        @assert ~isnan(avg_score)\n",
    "        history[epoch] = avg_score\n",
    "\n",
    "        if verbose\n",
    "            println(\"epoch $epoch: est. objective value: $avg_score\")\n",
    "        end\n",
    "        s2=time()-s1\n",
    "        println(\"Time to evaluate: $s2\")\n",
    "\n",
    "    end\n",
    "    return history\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = :amp2D\n",
    "for p in [:B_mu_mu,:B_logsigma_mu,:B_mu_sigma,:B_logsigma_sigma,\n",
    "        :B_mu_epsilon,:B_logsigma_epsilon]\n",
    "    Gen.init_param!(trainable_proposals[latent], p, zeros(4))\n",
    "end\n",
    "#Gaussians for scale_t:\n",
    "for p in [:W_mu_scale_t,:W_logsigma_scale_t]\n",
    "    Gen.init_param!(trainable_proposals[latent], p, zeros(3))\n",
    "end\n",
    "Gen.init_param!(trainable_proposals[latent], :mu_cats, [0.01,0.8,2.0])\n",
    "Gen.init_param!(trainable_proposals[latent], :logsigma_cats, [-1.0,-1.0,-1.0])\n",
    "#scale_f\n",
    "lf = length(get_element_gp_freqs(audio_sr, steps)); nf = sum(1:lf-1); \n",
    "for p in [:B_mu_scale_f, :B_logsigma_scale_f, :B1_mu_scale_f, :B1_logsigma_scale_f]\n",
    "    Gen.init_param!(trainable_proposals[latent], p, zeros(nf + 2))\n",
    "end\n",
    "for p in [:log_std_replace,\n",
    "        :C_mu_scale_t,:Bn_mu_scale_t,:replace_mu_scale_t,\n",
    "        :C_logsigma_scale_t,:Bn_logsigma_scale_t,:replace_logsigma_scale_t,\n",
    "        :replace_mu_epsilon,:replace_logsigma_epsilon]\n",
    "    Gen.init_param!(trainable_proposals[latent], p, 0.0)\n",
    "end\n",
    "update = Gen.ParamUpdate(GradientDescent(1e-8, 100), trainable_proposals[latent]);\n",
    "scores = pain!(trainable_proposals[latent], data_generators[latent], update,\n",
    "    num_epoch=100, epoch_size=5, num_minibatch=1, \n",
    "    minibatch_size=5, evaluation_size=400, verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(scores)\n",
    "xlabel(\"Iterations of stochastic gradient descent\")\n",
    "ylabel(\"Estimate of expected conditional log probability density\");\n",
    "title(string(\"GP: \", string(latent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in [:B_mu_mu,:B_logsigma_mu,:B_mu_sigma,:B_logsigma_sigma,\n",
    "        :B_mu_epsilon,:B_logsigma_epsilon,\n",
    "    :W_mu_scale_t,:W_logsigma_scale_t,\n",
    "    :B_mu_scale_f, :B_logsigma_scale_f, \n",
    "    :B1_mu_scale_f, :B1_logsigma_scale_f, :log_std_replace,\n",
    "        :C_mu_scale_t,:Bn_mu_scale_t,:replace_mu_scale_t,\n",
    "        :C_logsigma_scale_t,:Bn_logsigma_scale_t,:replace_logsigma_scale_t,\n",
    "        :replace_mu_epsilon,:replace_logsigma_epsilon]\n",
    "    s = string(p)\n",
    "    println(\"$s: \", Gen.get_param(trainable_proposals[latent], p))    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_trainable_gp2Dmu_proposal(latent; max_t=50, check_nt=1, check_nf=2, max_locals=50)\n",
    "\n",
    "    @gen function trainable_gp2Dmu_proposal(gp_elems)\n",
    "\n",
    "        #Mu and Sigma of GP\n",
    "        @param B_mu_mu::Matrix{Float64} #size=(f,f) ~ 400 \n",
    "        @param W_logsigma_mu::Matrix{Float64} # size=(f,f+2) ~440\n",
    "        @param C_logsigma_mu::Vector{Float64} #if Diagonal, size=f; if Matrix, size=(f,f) \n",
    "        @param log_std_replace::Vector{Float64} #if Diagonal, size=f; if Matrix size(f,f)\n",
    "        \n",
    "        mean_spectrum = mean(gp_elems[:reshaped],dims=2)[:,1]\n",
    "        mean_estimate = B_mu_mu*mean_spectrum\n",
    "        nt = size(gp_elems[:reshaped])[2] #min value = 1 \n",
    "        delta_t = gp_elems[:t][end]-gp_elems[:t][1] #min value = 0, how far data has travlled \n",
    "        if nt > 1\n",
    "            sig = std(gp_elems[:reshaped],dims=2)\n",
    "            s = cat(sig..., nt, delta_t, dims=1)\n",
    "            dia = exp.((W_logsigma_mu*s)[:,1] + C_logsigma_mu)\n",
    "        else\n",
    "            dia = exp.(log_std_replace)\n",
    "        end\n",
    "        gp_mu = @trace(mvnormal(mean_estimate, Diagonal(dia)), latent => :mu)\n",
    "        \n",
    "        #For the rest of the function, work with the de-mean'd elements\n",
    "        @param B_mu_sigma::Vector{Float64}\n",
    "        @param B_logsigma_sigma::Vector{Float64}        \n",
    "        gp_elems[:reshaped] = gp_elems[:reshaped] - repeat(reshape(gp_mu, length(gp_mu), 1), 1, nt)\n",
    "        sigma = std(gp_elems[:reshaped]);         \n",
    "        X = [sigma, length(gp_elems[:reshaped]), 1]\n",
    "        mu_sigma = dot(X, B_mu_sigma)\n",
    "        logsigma_sigma = dot(X, B_logsigma_sigma)\n",
    "        gp_sigma = @trace(log_normal(mu_sigma, exp(logsigma_sigma)), latent => :sigma)\n",
    "\n",
    "        #Temporal lengthscale of GP\n",
    "        @param W_mu_scale_t::Vector{Float64}\n",
    "        @param Bn_mu_scale_t::Float64\n",
    "        @param C_mu_scale_t::Float64\n",
    "        @param replace_mu_scale_t::Float64\n",
    "\n",
    "        @param W_logsigma_scale_t::Vector{Float64}\n",
    "        @param Bn_logsigma_scale_t::Float64\n",
    "        @param C_logsigma_scale_t::Float64\n",
    "        @param replace_logsigma_scale_t::Float64\n",
    "\n",
    "        @param mu_cats::Vector{Float64} #for categorizing time differences\n",
    "        @param logsigma_cats::Vector{Float64}\n",
    "\n",
    "        mu_scale_t = 0.0; logsigma_scale_t = 0.0; t = gp_elems[:t];\n",
    "        if length(t) > 1\n",
    "\n",
    "            #Subsample if GP is too big \n",
    "            idx = []; \n",
    "            if length(t) <= max_t\n",
    "                idx = 1:length(t) # 1 2 3 4\n",
    "            else\n",
    "                idx = Random.randperm(length(t)) # 4 7 2 1 8 5 3 6 10 9\n",
    "                idx = idx[1:max_t] # 4 7 2 1\n",
    "                idx = sort(idx) # 1 2 4 7 \n",
    "            end\n",
    "\n",
    "            n = sum(1:length(idx)-1) #how many pairs we will have \n",
    "            delta_ts = Vector{Float64}(undef, n)\n",
    "            delta_ys = Vector{Float64}(undef, n)\n",
    "            rs = Array{Float64}(undef, 1, n)\n",
    "            ps = [] #need to do it this way (rather than initialize an array) otherwise the gradients will break\n",
    "\n",
    "            k = 1\n",
    "            for i = 1:length(idx)\n",
    "                for j = (i+1):length(idx)\n",
    "                    i_idx = idx[i]; j_idx = idx[j]\n",
    "                    delta_ts[k] = t[j_idx] - t[i_idx]\n",
    "                    #Differences between 2D and 1D case\n",
    "                    p1 = gp_elems[:reshaped][:,j_idx] #Already de-mean'd\n",
    "                    p2 = gp_elems[:reshaped][:,i_idx] #Already de-mean'd\n",
    "                    delta_ys[k] = cor( p1 , p2 )\n",
    "                    #delta_ys[k] = sqrt(sum((p1 .- p2).^2)) \n",
    "                    rs[k] = delta_ys[k]/delta_ts[k]\n",
    "                    lp = [Gen.logpdf(normal, delta_ts[k], mu_cats[g],exp(logsigma_cats[g])) for g = 1:3]\n",
    "                    p = exp.(lp .- logsumexp(lp))\n",
    "                    push!(ps, p)\n",
    "                    k += 1\n",
    "                end\n",
    "            end\n",
    "            #cat(ps) is necessary to be able to compute the derivative given the list\n",
    "            #You need to use a list rather than editting a pre-made vector in place\n",
    "            ps = transpose(cat(ps...,dims=2))\n",
    "            mu_scale_t += (rs*ps*reshape(W_mu_scale_t, 3, 1))[1] + Bn_mu_scale_t*n + C_mu_scale_t #mu_scale(1)=rs(n,)*ps(n,3)*W(3,)\n",
    "            logsigma_scale_t += (rs*ps*reshape(W_logsigma_scale_t, 3, 1))[1] + Bn_logsigma_scale_t*n + C_logsigma_scale_t \n",
    "\n",
    "        else\n",
    "\n",
    "            mu_scale_t += replace_mu_scale_t\n",
    "            logsigma_scale_t += replace_logsigma_scale_t\n",
    "\n",
    "        end\n",
    "        @trace(log_normal(mu_scale_t, exp(logsigma_scale_t)), latent => :scale_t)\n",
    "\n",
    "        #Frequency lengthscale of GP\n",
    "        @param B_mu_scale_f::Vector{Float64} #len = sum(1:length(f)-1)+2\n",
    "        @param B1_mu_scale_f::Vector{Float64}\n",
    "        @param B_logsigma_scale_f::Vector{Float64}\n",
    "        @param B1_logsigma_scale_f::Vector{Float64}\n",
    "        \n",
    "        f = gp_elems[:f]; \n",
    "        rs = [];\n",
    "        for i = 1:length(f)\n",
    "            for j = i+1:length(f)\n",
    "                if length(t) > 1\n",
    "                    p1 = gp_elems[:reshaped][j,:] #Already de-mean'd\n",
    "                    p2 = gp_elems[:reshaped][i,:] #Already de-mean'd\n",
    "                    r = cor( p1, p2 )\n",
    "                else\n",
    "                    dy = abs(gp_elems[:reshaped][j,1] - gp_elems[:reshaped][i,1])\n",
    "#                     df = f[j] - f[i]\n",
    "                    r = dy/gp_sigma\n",
    "                end\n",
    "                push!(rs, r)  \n",
    "            end\n",
    "        end\n",
    "        #rcat is necessary to be able to compute the derivative given the list\n",
    "        #You need to use a list rather than editting a pre-made vector in place\n",
    "        rcat = cat(rs..., length(t), 1.0, dims=1) \n",
    "        Bm = length(t) > 1 ? B_mu_scale_f : B1_mu_scale_f\n",
    "        Bs = length(t) > 1 ? B_logsigma_scale_f : B1_logsigma_scale_f\n",
    "        mu_scale_f = dot(Bm,rcat)\n",
    "        logsigma_scale_f = dot(Bs,rcat)\n",
    "        @trace(log_normal(mu_scale_f, exp(logsigma_scale_f)), latent => :scale_f)\n",
    "        \n",
    "        ## Epsilon, local noise parameter of GP\n",
    "        #Check stds between closeby points only to get estimate\n",
    "        #Could also calculate the deviation of a middle-point\n",
    "        #from the straight line between its neighbours\n",
    "        @param B_mu_epsilon::Vector{Float64}\n",
    "        @param replace_mu_epsilon::Float64\n",
    "        @param B_logsigma_epsilon::Vector{Float64}\n",
    "        @param replace_logsigma_epsilon::Float64\n",
    "\n",
    "        local_stds = []; segment_size_t = 2*check_nt + 1; \n",
    "        for i = 1:segment_size_t:length(t) #It won't go beyond bounds, even if length(t) < segment_size\n",
    "            if issubset( [ t[i]+k*steps[\"t\"] for k = -check_nt:check_nt ], t )\n",
    "                j = rand(1+check_nf:length(f)-check_nf)\n",
    "                local_gp = gp_elems[:reshaped][j-check_nf:j+check_nf, i-check_nt:i+check_nt][:] #contiguous GP points\n",
    "                push!(local_stds, std(local_gp))\n",
    "            end\n",
    "            if length(local_stds) == max_locals\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        mu_epsilon = 0.0; logsigma_epsilon = 0.0;\n",
    "        if length(local_stds) > 1\n",
    "            mu_locals = mean(local_stds)\n",
    "            std_locals = std(local_stds)\n",
    "            X = [mu_locals, std_locals, length(local_stds), 1.0]\n",
    "\n",
    "            mu_epsilon += dot(X, B_mu_epsilon)\n",
    "            logsigma_epsilon += dot(X, B_logsigma_epsilon)\n",
    "        else\n",
    "            mu_epsilon += replace_mu_epsilon\n",
    "            logsigma_epsilon += replace_logsigma_epsilon\n",
    "        end\n",
    "        @trace(log_normal(mu_epsilon, exp(logsigma_epsilon)), latent => :epsilon)\n",
    "\n",
    "        return nothing\n",
    "    end\n",
    "\n",
    "    return trainable_gp2Dmu_proposal\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_source_params = source_params\n",
    "noise_mu_params = Dict(:sigma=> 10.0, :scale=> 4.0, :epsilon=> 1.0)\n",
    "f = get_element_gp_freqs(audio_sr, steps)\n",
    "c = generate_covariance_matrix(f, noise_mu_params)\n",
    "spectrum_source_params[\"gp\"][\"amp\"][\"2D\"][\"mu\"][\"dist\"]=mvnormal\n",
    "spectrum_source_params[\"gp\"][\"amp\"][\"2D\"][\"mu\"][\"args\"]=(fill(10.0,length(f)),c)\n",
    "spectrum_latent_model = make_source_latent_model(spectrum_source_params, audio_sr, steps, scene_duration)\n",
    "\n",
    "evaluation_size = 400\n",
    "latents[:spectrum] = Dict(:gp => :amp, :source_type => \"noise\")\n",
    "data_generators[:spectrum] = make_data_generator(spectrum_latent_model, latents[:spectrum])\n",
    "trainable_proposals[:spectrum] = create_trainable_gp2Dmu_proposal(:amp);\n",
    "ic = Dict()\n",
    "ic[:spectrum] = [data_generators[:spectrum]() for i = 1:evaluation_size];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Made \", length(ic[:spectrum]), \" datapoints for eval.\")\n",
    "#size(data_generators[:spectrum]()[2][:amp => :mu]) = (9,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = :spectrum\n",
    "#spectrum mean\n",
    "lf = length(get_element_gp_freqs(audio_sr, steps)); \n",
    "Gen.init_param!(trainable_proposals[latent], :B_mu_mu, zeros(lf,lf) + Diagonal(fill(1e-5,lf)))\n",
    "Gen.init_param!(trainable_proposals[latent], :W_logsigma_mu, zeros(lf, lf+2) + cat(Diagonal(fill(1e-5,lf)),1e-5*ones(lf,2),dims=2))\n",
    "Gen.init_param!(trainable_proposals[latent], :C_logsigma_mu, zeros(lf))\n",
    "Gen.init_param!(trainable_proposals[latent], :log_std_replace, zeros(lf))\n",
    "#local epsilon noise\n",
    "for p in [:B_mu_epsilon,:B_logsigma_epsilon]\n",
    "    Gen.init_param!(trainable_proposals[latent], p, zeros(4))\n",
    "end\n",
    "#sigma of GP kernel & Gaussians for scale_t:\n",
    "for p in [:B_mu_sigma,:B_logsigma_sigma,\n",
    "          :W_mu_scale_t,:W_logsigma_scale_t]\n",
    "    Gen.init_param!(trainable_proposals[latent], p, zeros(3))\n",
    "end\n",
    "Gen.init_param!(trainable_proposals[latent], :mu_cats, [0.01,0.8,2.0])\n",
    "Gen.init_param!(trainable_proposals[latent], :logsigma_cats, [-1.0,-1.0,-1.0])\n",
    "#scale_f\n",
    "nf = sum(1:lf-1); \n",
    "for p in [:B_mu_scale_f, :B_logsigma_scale_f, :B1_mu_scale_f, :B1_logsigma_scale_f]\n",
    "    Gen.init_param!(trainable_proposals[latent], p, zeros(nf + 2))\n",
    "end\n",
    "for p in [:C_mu_scale_t,:Bn_mu_scale_t,:replace_mu_scale_t,\n",
    "        :C_logsigma_scale_t,:Bn_logsigma_scale_t,:replace_logsigma_scale_t,\n",
    "        :replace_mu_epsilon,:replace_logsigma_epsilon]\n",
    "    Gen.init_param!(trainable_proposals[latent], p, 0.0)\n",
    "end\n",
    "update = Gen.ParamUpdate(GradientDescent(1e-8, 100), trainable_proposals[latent]); #1e-8\n",
    "scores = pain!(trainable_proposals[latent], data_generators[latent], update,\n",
    "    num_epoch=1000, epoch_size=5, num_minibatch=1, \n",
    "    minibatch_size=5, evaluation_size=400, eval_inputs_and_constraints=ic[latent], verbose=true)\n",
    "\n",
    "plot(scores)\n",
    "xlabel(\"Iterations of stochastic gradient descent\")\n",
    "ylabel(\"Estimate of expected conditional log probability density\");\n",
    "title(string(\"GP: \", string(latent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in [:B_mu_mu,:W_logsigma_mu,:C_logsigma_mu,:log_std_replace,\n",
    "    :B_mu_epsilon,:B_logsigma_epsilon,\n",
    "    :B_mu_sigma,:B_logsigma_sigma,:W_mu_scale_t,:W_logsigma_scale_t,\n",
    "        :mu_cats,:logsigma_cats,\n",
    "    :B_mu_scale_f, :B_logsigma_scale_f, :B1_mu_scale_f, :B1_logsigma_scale_f,\n",
    "    :log_std_replace,\n",
    "        :C_mu_scale_t,:Bn_mu_scale_t,:replace_mu_scale_t,\n",
    "        :C_logsigma_scale_t,:Bn_logsigma_scale_t,:replace_logsigma_scale_t,\n",
    "        :replace_mu_epsilon,:replace_logsigma_epsilon]\n",
    "    s = string(p)\n",
    "    println(\"$s: \", Gen.get_param(trainable_proposals[latent], p))    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  [9.04086e-6 2.44144e-5 4.27344e-5 -6.39516e-5 2.30337e-5 5.08731e-5 3.33898e-5 9.96364e-7 -4.10042e-5; -1.45731e-6 2.34339e-5 5.1541e-5 -2.76425e-5 3.94316e-5 6.06721e-5 3.30423e-5 -4.3137e-6 -3.0916e-5; -1.72753e-5 2.34302e-5 9.49068e-5 -2.84909e-5 8.27286e-6 9.26835e-7 2.58571e-5 -1.45681e-5 -2.27462e-5; 6.87276e-8 3.88148e-5 7.13896e-5 -4.49074e-5 8.19337e-6 8.52178e-6 4.72579e-5 -1.79114e-5 5.91304e-6; -9.16937e-6 4.75617e-5 5.14232e-5 -3.78198e-5 3.42397e-5 -1.04747e-5 8.92965e-6 -1.74021e-5 3.83545e-5; -2.48914e-5 2.84504e-5 5.00691e-5 -3.7815e-5 3.12402e-5 7.34944e-6 1.09918e-5 -5.00089e-6 3.13616e-5; -2.44663e-5 -1.41264e-5 5.41902e-5 -3.58634e-5 4.40222e-5 3.12245e-5 2.85718e-5 1.25084e-5 -2.69875e-6; -4.60631e-5 -4.45336e-5 3.49122e-5 -3.78671e-5 1.43824e-5 1.97026e-5 7.41281e-6 2.50954e-5 -6.98757e-6; -3.80551e-5 -3.90626e-6 3.39495e-5 -2.45203e-5 7.07236e-6 -1.01003e-6 -1.64635e-5 2.10555e-5 2.16596e-5]\n",
    "plt.imshow(x)\n",
    "# y = [0.00756103 0.00789582 0.00861523 0.00837371 0.00816384 0.00770816 0.00873847 0.00829039 0.00851563 0.0255121 0.00129844; 0.00756103 0.00789582 0.00861523 0.00837371 0.00816384 0.00770816 0.00873847 0.00829039 0.00851563 0.0255121 0.00129844; 0.00756103 0.00789582 0.00861523 0.00837371 0.00816384 0.00770816 0.00873847 0.00829039 0.00851563 0.0255121 0.00129844; 0.00756103 0.00789582 0.00861523 0.00837371 0.00816384 0.00770816 0.00873847 0.00829039 0.00851563 0.0255121 0.00129844; 0.00756103 0.00789582 0.00861523 0.00837371 0.00816384 0.00770816 0.00873847 0.00829039 0.00851563 0.0255121 0.00129844; 0.00756103 0.00789582 0.00861523 0.00837371 0.00816384 0.00770816 0.00873847 0.00829039 0.00851563 0.0255121 0.00129844; 0.00756103 0.00789582 0.00861523 0.00837371 0.00816384 0.00770816 0.00873847 0.00829039 0.00851563 0.0255121 0.00129844; 0.00756103 0.00789582 0.00861523 0.00837371 0.00816384 0.00770816 0.00873847 0.00829039 0.00851563 0.0255121 0.00129844; 0.00756103 0.00789582 0.00861523 0.00837371 0.00816384 0.00770816 0.00873847 0.00829039 0.00851563 0.0255121 0.00129844]\n",
    "# plt.imshow(y)\n",
    "x = [-2.69401e-5 -8.06477e-6 -5.79068e-6 -3.43522e-5 2.64049e-5 1.51815e-5 -4.44227e-5 -1.55228e-5 1.84711e-5; -2.07865e-5 8.60812e-6 -5.41509e-6 -7.2196e-6 2.63588e-5 1.44334e-5 -3.13472e-5 -3.41347e-5 8.26784e-6; -2.2381e-5 4.44332e-6 4.57912e-5 2.86229e-5 4.57247e-5 2.26134e-5 -7.31034e-5 -5.7595e-5 2.98627e-5; 1.49586e-6 -3.62218e-6 6.22207e-5 5.23283e-5 9.82693e-5 6.75616e-5 -6.54816e-5 -6.67691e-5 2.78072e-5; -8.30655e-6 -2.1471e-5 1.83352e-5 1.761e-5 0.000114502 5.75503e-5 -9.3877e-5 -9.90538e-5 1.17878e-5; -4.07757e-5 8.22624e-6 2.42239e-5 1.29366e-5 7.45208e-5 5.29256e-5 -9.25707e-5 -7.30924e-5 7.57786e-6; -3.50436e-5 3.9368e-5 5.19086e-5 6.72993e-6 5.94673e-5 6.39709e-5 -8.69786e-5 -4.99406e-5 2.43322e-5; 2.03586e-6 1.90803e-5 2.6395e-5 2.27187e-5 4.6972e-5 3.21086e-5 -6.9864e-5 -8.75867e-6 5.25513e-5; -5.47772e-6 -2.13026e-5 -7.34398e-6 3.37024e-5 6.27379e-5 1.87207e-5 -1.681e-5 -1.02412e-5 2.97232e-5]\n",
    "plt.imshow(x)\n",
    "x = [0.00482351 0.00482301 0.00475211 0.00490983 0.00477718 0.00485861 0.00492406 0.004856 0.00498754 0.0178249 0.00069147; 0.00510025 0.00507656 0.00494271 0.00520305 0.00506702 0.00518367 0.00531098 0.00513295 0.00525123 0.0181254 0.000711474; 0.00500333 0.00506287 0.00486013 0.00509223 0.00499615 0.00494119 0.00515581 0.00512848 0.00503768 0.0179779 0.000720386; 0.00498367 0.00489816 0.00475871 0.00496714 0.00479857 0.00480101 0.00491759 0.00493257 0.00489266 0.0179533 0.000708448; 0.0051403 0.0050187 0.00499268 0.00497967 0.00491829 0.00496581 0.00508766 0.00517086 0.00511394 0.0178707 0.000709368; 0.0051091 0.00507743 0.00508256 0.00502558 0.00515143 0.00516184 0.00518949 0.00520754 0.00513222 0.0176809 0.000690717; 0.00516648 0.00510071 0.00510093 0.00502205 0.00509871 0.00508232 0.00514596 0.00514076 0.005121 0.0175336 0.000708668; 0.00519239 0.00511753 0.00509234 0.00503172 0.00501836 0.00511156 0.00514286 0.00523219 0.00531263 0.017629 0.000813126; 0.00528517 0.00522148 0.00517495 0.00510391 0.0050347 0.00506368 0.00511372 0.00529983 0.00532528 0.0179866 0.000790359]\n",
    "plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent=:spectrum\n",
    "rs = []\n",
    "results = Dict(:scale_t=>Dict(:c=>[], :t=>[]),\n",
    "                :scale_f=>Dict(:c=>[], :t=>[]),\n",
    "                :sigma=>Dict(:c=>[], :t=>[]),\n",
    "                :epsilon=>Dict(:c=>[],:t=>[]))\n",
    "for z = 1:100\n",
    "    i,c = data_generators[latent]()\n",
    "    trace, _, _ = propose(trainable_proposals[latent],(i[1],))\n",
    "    push!(rs,cor(c[:amp=>:mu],trace[:amp=>:mu]))\n",
    "    for a in keys(results)\n",
    "        push!(results[a][:c],c[:amp=>a])\n",
    "        push!(results[a][:t],trace[:amp=>a])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(rs)\n",
    "title(string(\"Correlation between sampled mu and predicted, mean: \", round(mean(rs),digits=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(results[:scale_t][:c],results[:scale_t][:t])\n",
    "xlabel(\"Sampled scale_t\")\n",
    "ylabel(\"Predicted scale_t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(results[:scale_f][:c],results[:scale_f][:t])\n",
    "xlabel(\"Sampled scale_f\")\n",
    "ylabel(\"Predicted scale_f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(results[:epsilon][:c],results[:epsilon][:t])\n",
    "xlabel(\"Sampled epsilon\")\n",
    "ylabel(\"Predicted epsilon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(results[:sigma][:c],results[:sigma][:t])\n",
    "xlabel(\"Sampled sigma\")\n",
    "ylabel(\"Predicted sigma\")\n",
    "ylim([0,45])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions with the learned params built in, so that they can be used deterministically inside of involution calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IJulia.set_verbose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[1.0 2.53617e-8 1.66455e-8 4.6655e-8 1.08739e-8 5.86392e-9 4.55852e-8 -2.69102e-8 3.74473e-10; -2.87734e-9 1.0 8.10441e-9 1.6443e-8 3.66857e-9 4.11638e-10 1.52253e-8 -1.11827e-8 -7.00062e-10; -4.06295e-8 -1.05507e-8 1.0 -1.96244e-8 -1.77769e-9 2.61952e-8 -4.8047e-9 4.08587e-8 1.43087e-8; -2.41308e-8 -2.69301e-8 -5.15402e-8 1.0 -9.15042e-9 1.86673e-8 -3.60519e-8 5.43367e-8 1.21603e-8; 9.28191e-9 -1.15712e-9 6.31115e-9 -2.08482e-9 1.0 -5.46661e-9 -4.66172e-9 -4.1514e-9 -2.63898e-9; 6.78281e-8 3.49587e-8 9.70636e-8 6.47599e-8 9.74444e-9 1.0 3.57981e-8 -9.34781e-8 -2.69687e-8; 3.35265e-8 1.0935e-8 4.05314e-8 2.03186e-8 2.32841e-9 -2.19906e-8 1.0 -3.69765e-8 -1.2207e-8; 6.57322e-8 1.02229e-7 1.74281e-7 1.88714e-7 3.62453e-8 -5.51601e-8 1.44639e-7 1.0 -3.82369e-8; 5.12542e-8 6.02737e-8 1.13079e-7 1.11325e-7 2.0641e-8 -4.01075e-8 8.15196e-8 -1.19883e-7 1.0]\n",
    "plt.imshow(x.-Diagonal(ones(9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deterministic_proposals=Dict()\n",
    "\n",
    "function create_deterministic_tp_proposal(latent, B_mu_mu::Vector{Float64}, B_logsigma_mu::Vector{Float64},\n",
    "    B_mu_alpha::Vector{Float64}, B_logsigma_alpha::Vector{Float64}, log_std_replace::Float64)\n",
    "\n",
    "    function deterministic_tp_proposal(tp_elems)\n",
    "\n",
    "        n = length(tp_elems[latent])\n",
    "        mu = mean(tp_elems[latent])\n",
    "        sigma = n > 1 ? std(tp_elems[latent]) : exp(log_std_replace)\n",
    "        X = [mu, sigma, n, 1]\n",
    "\n",
    "        mu_mu = dot(X, B_mu_mu)\n",
    "        logsigma_mu = dot(X, B_logsigma_mu)\n",
    "        mu_alpha = dot(X, B_mu_alpha)\n",
    "        logsigma_alpha = dot(X, B_logsigma_alpha)\n",
    "\n",
    "        return Dict(latent => \n",
    "                    Dict(:mu => Dict(\"dist\"=>log_normal,\"args\"=>(mu_mu, exp(logsigma_mu)))),\n",
    "                    Dict(:a => Dict(\"dist\"=>log_normal, \"args\"=>(mu_alpha, exp(logsigma_alpha))))\n",
    "                    )\n",
    "\n",
    "    end\n",
    "    \n",
    "    return deterministic_tp_proposal\n",
    "    \n",
    "end\n",
    "\n",
    "function create_deterministic_gp1D_proposal(latent,B_mu_mu::Vector{Float64},B_logsigma_mu::Vector{Float64},B_mu_sigma::Vector{Float64},\n",
    "         B_logsigma_sigma::Vector{Float64},log_std_replace::Float64, W_mu_scale::Vector{Float64},\n",
    "        Bn_mu_scale::Float64,C_mu_scale::Float64,replace_mu_scale::Float64,W_logsigma_scale::Vector{Float64},\n",
    "        Bn_logsigma_scale::Float64,C_logsigma_scale::Float64,replace_logsigma_scale::Float64,\n",
    "        mu_cats::Vector{Float64},logsigma_cats::Vector{Float64}, B_mu_epsilon::Vector{Float64},\n",
    "        replace_mu_epsilon::Float64, B_logsigma_epsilon::Vector{Float64},\n",
    "        replace_logsigma_epsilon::Float64; max_t=50, check_n=2, max_locals=50)\n",
    "        \n",
    "    function deterministic_gp1D_proposal(gp_elems)\n",
    "\n",
    "        #Mu and Sigma of GP\n",
    "        mu = mean(gp_elems[latent])\n",
    "        sigma = length(gp_elems[latent]) > 1 ? std(gp_elems[latent]) : exp(log_std_replace)\n",
    "        X = [mu, sigma, length(gp_elems[latent]), 1]\n",
    "\n",
    "        mu_mu = dot(X, B_mu_mu)\n",
    "        logsigma_mu = dot(X, B_logsigma_mu)\n",
    "        mu_sigma = dot(X, B_mu_sigma)\n",
    "        logsigma_sigma = dot(X, B_logsigma_sigma)\n",
    "\n",
    "        #Temporal lengthscale of GP\n",
    "        mu_scale = 0.0; logsigma_scale = 0.0; t = gp_elems[:t];\n",
    "        if length(t) > 1\n",
    "\n",
    "            #Subsample if GP is too big \n",
    "            idx = []; \n",
    "            if length(t) <= max_t\n",
    "                idx = 1:length(t) # 1 2 3 4\n",
    "            else\n",
    "                idx = Random.randperm(length(t)) # 4 7 2 1 8 5 3 6 10 9\n",
    "                idx = idx[1:max_t] # 4 7 2 1\n",
    "                idx = sort(idx) # 1 2 4 7 \n",
    "            end\n",
    "\n",
    "            n = sum(1:length(idx)-1) #how many pairs we will have \n",
    "            delta_ts = Vector{Float64}(undef, n)\n",
    "            delta_ys = Vector{Float64}(undef, n)\n",
    "            rs = Array{Float64}(undef, 1, n)\n",
    "            ps = [] #need to do it this way (rather than initialize an array) otherwise the gradients will break\n",
    "\n",
    "            k = 1\n",
    "            for i = 1:length(idx)\n",
    "                for j = (i+1):length(idx)\n",
    "                    i_idx = idx[i]; j_idx = idx[j]\n",
    "                    delta_ts[k] = t[j_idx] - t[i_idx]\n",
    "                    delta_ys[k] = abs(gp_elems[latent][j_idx] - gp_elems[latent][i_idx])\n",
    "                    ##This is an issue! you need to sample sigma before you compute the other ones. \n",
    "                    rs[k] = delta_ys[k]/(delta_ts[k]*gp_sigma)\n",
    "                    lp = [Gen.logpdf(normal, delta_ts[k], mu_cats[g],exp(logsigma_cats[g])) for g = 1:3]\n",
    "                    p = exp.(lp .- logsumexp(lp))\n",
    "                    push!(ps, p)\n",
    "                    k += 1\n",
    "                end\n",
    "            end\n",
    "\n",
    "            ps = transpose(cat(ps...,dims=2))\n",
    "            mu_scale += (rs*ps*reshape(W_mu_scale, 3, 1))[1] + Bn_mu_scale*n + C_mu_scale #mu_scale(1)=rs(n,)*ps(n,3)*W(3,)\n",
    "            logsigma_scale += (rs*ps*reshape(W_logsigma_scale, 3, 1))[1] + Bn_logsigma_scale*n + C_logsigma_scale \n",
    "\n",
    "        else\n",
    "\n",
    "            mu_scale += replace_mu_scale\n",
    "            logsigma_scale += replace_logsigma_scale\n",
    "\n",
    "        end\n",
    "        \n",
    "        ## Epsilon, local noise parameter of GP\n",
    "        #Check stds between closeby points only to get estimate\n",
    "        #Could also calculate the deviation of a middle-point\n",
    "        #from the straight line between its neighbours\n",
    "\n",
    "        local_stds = []; segment_size = 2*check_n + 1\n",
    "        for i = 1:segment_size:length(t) #It won't go beyond bounds, even if length(t) < segment_size\n",
    "            if issubset( [ t[i]+k*steps[\"t\"] for k = -check_n:check_n ], t )\n",
    "                local_gp = gp_elems[latent][i-check_n:i+check_n] #contiguous GP points\n",
    "                push!(local_stds, std(local_gp))\n",
    "            end\n",
    "            if length(local_stds) == max_locals\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        mu_epsilon = 0.0; logsigma_epsilon = 0.0;\n",
    "        if length(local_stds) > 1\n",
    "            mu_locals = mean(local_stds)\n",
    "            std_locals = std(local_stds)\n",
    "            X = [mu_locals, std_locals, length(local_stds), 1]\n",
    "\n",
    "            mu_epsilon += dot(X, B_mu_epsilon)\n",
    "            logsigma_epsilon += dot(X, B_logsigma_epsilon)\n",
    "        else\n",
    "            mu_epsilon += replace_mu_epsilon\n",
    "            logsigma_epsilon += replace_logsigma_epsilon\n",
    "        end\n",
    "        \n",
    "        return Dict(latent => Dict(:mu => Dict(\"dist\"=>normal, args=>(mu_mu, exp(logsigma_mu))),\n",
    "                            :sigma => Dict(\"dist\"=>log_normal, args=>(mu_sigma, exp(logsigma_sigma))),\n",
    "                            :scale => Dict(\"dist\"=>log_normal, args=>(mu_scale, exp(logsigma_scale))),\n",
    "                            :epsilon => Dict(\"dist\"=>log_normal, args=>(mu_epsilon, exp(logsigma_epsilon)))\n",
    "                            \n",
    "                ))\n",
    "\n",
    "    end\n",
    "\n",
    "    return deterministic_gp1D_proposal\n",
    "    \n",
    "end\n",
    "\n",
    "function create_deterministic_gp2D_proposal(latent; max_t=5, check_nt=1, check_nf=2, max_locals=5)\n",
    "\n",
    "        @param B_mu_mu::Vector{Float64}\n",
    "        @param B_logsigma_mu::Vector{Float64}\n",
    "        @param B_mu_sigma::Vector{Float64}\n",
    "        @param B_logsigma_sigma::Vector{Float64}\n",
    "    \n",
    "            @param W_mu_scale_t::Vector{Float64}\n",
    "        @param Bn_mu_scale_t::Float64\n",
    "        @param C_mu_scale_t::Float64\n",
    "        @param replace_mu_scale_t::Float64\n",
    "\n",
    "        @param W_logsigma_scale_t::Vector{Float64}\n",
    "        @param Bn_logsigma_scale_t::Float64\n",
    "        @param C_logsigma_scale_t::Float64\n",
    "        @param replace_logsigma_scale_t::Float64\n",
    "\n",
    "        @param mu_cats::Vector{Float64} #for categorizing time differences\n",
    "        @param logsigma_cats::Vector{Float64}\n",
    "\n",
    "    \n",
    "    function deterministic_gp2D_proposal(gp_elems)\n",
    "\n",
    "        #Mu and Sigma of GP\n",
    "        mu = mean(gp_elems[latent])\n",
    "        sigma = std(gp_elems[latent])\n",
    "        X = [mu, sigma, length(gp_elems[latent]), 1]\n",
    "\n",
    "        mu_mu = dot(X, B_mu_mu)\n",
    "        logsigma_mu = dot(X, B_logsigma_mu)\n",
    "        mu_sigma = dot(X, B_mu_sigma)\n",
    "        logsigma_sigma = dot(X, B_logsigma_sigma)\n",
    "\n",
    "        gp_mu = @trace(normal(mu_mu,exp(logsigma_mu)), latent => :mu)\n",
    "        gp_sigma = @trace(log_normal(mu_sigma, exp(logsigma_sigma)), latent => :sigma)\n",
    "        gp_elems[:reshaped] = gp_elems[:reshaped] .- gp_mu #de-mean\n",
    "\n",
    "        #Temporal lengthscale of GP\n",
    "        mu_scale_t = 0.0; logsigma_scale_t = 0.0; t = gp_elems[:t];\n",
    "        if length(t) > 1\n",
    "\n",
    "            #Subsample if GP is too big \n",
    "            idx = []; \n",
    "            if length(t) <= max_t\n",
    "                idx = 1:length(t) # 1 2 3 4\n",
    "            else\n",
    "                idx = Random.randperm(length(t)) # 4 7 2 1 8 5 3 6 10 9\n",
    "                idx = idx[1:max_t] # 4 7 2 1\n",
    "                idx = sort(idx) # 1 2 4 7 \n",
    "            end\n",
    "\n",
    "            n = sum(1:length(idx)-1) #how many pairs we will have \n",
    "            delta_ts = Vector{Float64}(undef, n)\n",
    "            delta_ys = Vector{Float64}(undef, n)\n",
    "            rs = []#Array{Float64}(undef, 1, n)\n",
    "            ps = [] #need to do it this way (rather than initialize an array) otherwise the gradients will break\n",
    "\n",
    "            k = 1\n",
    "            for i = 1:length(idx)\n",
    "                for j = (i+1):length(idx)\n",
    "                    i_idx = idx[i]; j_idx = idx[j]\n",
    "                    delta_ts[k] = t[j_idx] - t[i_idx]\n",
    "                    #Differences between 2D and 1D case\n",
    "                    push!(rs, cor( gp_elems[:reshaped][:,j_idx] , gp_elems[:reshaped][:,i_idx] ))\n",
    "                    #delta_ys[k] = sqrt(sum((p1 .- p2).^2)) \n",
    "                    #rs[k] = delta_ys[k]/delta_ts[k]\n",
    "                    lp = [Gen.logpdf(normal, delta_ts[k], mu_cats[g],exp(logsigma_cats[g])) for g = 1:3]\n",
    "                    p = exp.(lp .- logsumexp(lp))\n",
    "                    push!(ps, p)\n",
    "                    k += 1\n",
    "                end\n",
    "            end\n",
    "            #cat(ps) is necessary to be able to compute the derivative given the list\n",
    "            #You need to use a list rather than editting a pre-made vector in place\n",
    "            ps = transpose(cat(ps...,dims=2))\n",
    "            rs = reshape(cat(rs...,dims=1), 1, n)\n",
    "            mu_scale_t += (rs*ps*reshape(W_mu_scale_t, 3, 1))[1] + Bn_mu_scale_t*n + C_mu_scale_t #mu_scale(1)=rs(n,)*ps(n,3)*W(3,)\n",
    "            logsigma_scale_t += (rs*ps*reshape(W_logsigma_scale_t, 3, 1))[1] + Bn_logsigma_scale_t*n + C_logsigma_scale_t \n",
    "\n",
    "        else\n",
    "\n",
    "            mu_scale_t += replace_mu_scale_t\n",
    "            logsigma_scale_t += replace_logsigma_scale_t\n",
    "\n",
    "        end\n",
    "        @trace(log_normal(mu_scale_t, exp(logsigma_scale_t)), latent => :scale_t)\n",
    "\n",
    "        #Frequency lengthscale of GP\n",
    "        @param B_mu_scale_f::Vector{Float64} #len = sum(1:length(f)-1)+2\n",
    "        @param B1_mu_scale_f::Vector{Float64}\n",
    "        @param B_logsigma_scale_f::Vector{Float64}\n",
    "        @param B1_logsigma_scale_f::Vector{Float64}\n",
    "        \n",
    "        f = gp_elems[:f]; \n",
    "        rs = [];\n",
    "        for i = 1:length(f)\n",
    "            for j = i+1:length(f)\n",
    "                if length(t) > 1\n",
    "                    r = cor( gp_elems[:reshaped][j,:], gp_elems[:reshaped][i,:] )\n",
    "                else\n",
    "                    dy = abs(gp_elems[:reshaped][j,1] - gp_elems[:reshaped][i,1])\n",
    "#                     df = f[j] - f[i]\n",
    "                    r = dy/gp_sigma\n",
    "                end\n",
    "                push!(rs, r)  \n",
    "            end\n",
    "        end\n",
    "        #rcat is necessary to be able to compute the derivative given the list\n",
    "        #You need to use a list rather than editting a pre-made vector in place\n",
    "        rcat = cat(rs..., length(t), 1.0, dims=1) \n",
    "        Bm = length(t) > 1 ? B_mu_scale_f : B1_mu_scale_f\n",
    "        Bs = length(t) > 1 ? B_logsigma_scale_f : B1_logsigma_scale_f\n",
    "        mu_scale_f = dot(Bm,rcat)\n",
    "        logsigma_scale_f = dot(Bs,rcat)\n",
    "        @trace(log_normal(mu_scale_f, exp(logsigma_scale_f)), latent => :scale_f)\n",
    "        \n",
    "        ## Epsilon, local noise parameter of GP\n",
    "        #Check stds between closeby points only to get estimate\n",
    "        #Could also calculate the deviation of a middle-point\n",
    "        #from the straight line between its neighbours\n",
    "        @param B_mu_epsilon::Vector{Float64}\n",
    "        @param replace_mu_epsilon::Float64\n",
    "        @param B_logsigma_epsilon::Vector{Float64}\n",
    "        @param replace_logsigma_epsilon::Float64\n",
    "\n",
    "        local_stds = []; segment_size_t = 2*check_nt + 1; \n",
    "        for i = 1:segment_size_t:length(t) #It won't go beyond bounds, even if length(t) < segment_size\n",
    "            if issubset( [ t[i]+k*steps[\"t\"] for k = -check_nt:check_nt ], t )\n",
    "                j = rand(1+check_nf:length(f)-check_nf)\n",
    "                local_gp = gp_elems[:reshaped][j-check_nf:j+check_nf, i-check_nt:i+check_nt][:] #contiguous GP points\n",
    "                push!(local_stds, std(local_gp))\n",
    "            end\n",
    "            if length(local_stds) == max_locals\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        mu_epsilon = 0.0; logsigma_epsilon = 0.0;\n",
    "        if length(local_stds) > 1\n",
    "            mu_locals = mean(local_stds)\n",
    "            std_locals = std(local_stds)\n",
    "            X = [mu_locals, std_locals, length(local_stds), 1.0]\n",
    "\n",
    "            mu_epsilon += dot(X, B_mu_epsilon)\n",
    "            logsigma_epsilon += dot(X, B_logsigma_epsilon)\n",
    "        else\n",
    "            mu_epsilon += replace_mu_epsilon\n",
    "            logsigma_epsilon += replace_logsigma_epsilon\n",
    "        end\n",
    "        @trace(log_normal(mu_epsilon, exp(logsigma_epsilon)), latent => :epsilon)\n",
    "\n",
    "        return nothing\n",
    "\n",
    "    end\n",
    "\n",
    "    return trainable_gp2D_proposal\n",
    "    \n",
    "end"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
